# P2.7 Gestion du Contexte LLM

## Objectif

Optimiser la gestion du contexte envoyÃ© au LLM pour :

- RÃ©duire les coÃ»ts (moins de tokens)
- Ã‰viter les hallucinations (contexte pertinent uniquement)
- Maintenir la cohÃ©rence (pas de perte de mÃ©moire)
- Supporter les sessions longues

## PrÃ©requis

- [x] P1.1 - Structure globale
- [x] P1.2 - Project init avec context.md
- [ ] P2.6 - Session management

## Ce qui EXISTE dÃ©jÃ  (Ã  Ã©tendre)

> **IMPORTANT** : Ne pas recrÃ©er ce qui fonctionne !

### Backend

- `server/services/context_discovery.py` â€” Scanne le projet pour GDD, docs, architecture
- `server/services/workflow/prompt_builder.py` â€” Construction des prompts avec contexte
- `server/services/llm.py` â€” Gestion multi-provider avec streaming

### Ce qui MANQUE (Ã  ajouter)

- Contexte hiÃ©rarchique (CORE / RELEVANT / DÃ‰TAILS)
- Extraction de facts structurÃ©s (pas texte brut)
- Injection sÃ©lective par Ã©tape workflow
- RÃ©sumÃ© progressif des longues conversations
- Prompt caching Anthropic

## ProblÃ¨me

Sans stratÃ©gie, chaque appel LLM envoie tout :

```
âŒ Contexte non optimisÃ© (par appel)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ System prompt agent (~500 tokens)   â”‚
â”‚ GDD complet (~5000 tokens)          â”‚
â”‚ Brief complet (~2000 tokens)        â”‚
â”‚ Historique conversation (~3000+)    â”‚
â”‚ RÃ©ponses workflow (~1000+)          â”‚
â”‚ Derniers messages (~500)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL: ~12000+ tokens par appel     â”‚
â”‚ = CoÃ»t Ã©levÃ© + risque hallucination â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Solution : Contexte HiÃ©rarchique

```
âœ… Contexte optimisÃ© (par appel)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  NIVEAU 1 â€” CORE (toujours prÃ©sent)           ~500 tokens  â”‚
â”‚  â”œâ”€â”€ IdentitÃ© projet (nom, genre, pitch)                   â”‚
â”‚  â”œâ”€â”€ Agent actif (persona courte)                          â”‚
â”‚  â””â”€â”€ Ã‰tat actuel (workflow, Ã©tape, question)               â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  NIVEAU 2 â€” RELEVANT (selon l'Ã©tape)          ~1000 tokens â”‚
â”‚  â”œâ”€â”€ Facts extraits (pas le texte brut)                    â”‚
â”‚  â”œâ”€â”€ RÃ©ponses des Ã©tapes liÃ©es                             â”‚
â”‚  â””â”€â”€ DÃ©cisions clÃ©s prises                                 â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  NIVEAU 3 â€” DÃ‰TAILS (lazy-load Ã  la demande)  ~variable   â”‚
â”‚  â”œâ”€â”€ GDD section spÃ©cifique (si pertinent)                 â”‚
â”‚  â”œâ”€â”€ Historique conversation rÃ©sumÃ©                        â”‚
â”‚  â””â”€â”€ Sessions prÃ©cÃ©dentes                                  â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TOTAL TYPIQUE: ~2000-3000 tokens (vs 12000+)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## SpÃ©cifications

### 1. Structure du Contexte

```yaml
# .unreal-companion/.state/context-state.yaml
project:
  name: "The Last Shard"
  genre: "exploration"
  pitch: "Explorer un monde figÃ©..."  # Max 100 chars

current:
  mode: studio  # studio | editor
  agent: game-designer
  workflow: game-brief
  step: 3
  question: core_mechanics

# Facts = donnÃ©es structurÃ©es extraites des rÃ©ponses
facts:
  vision:
    pitch: "Explorer un monde figÃ© dans le temps"
    fantasy: "ArchÃ©ologue linguistique"
    extracted_at: "2024-01-22T10:00:00"

  genre:
    primary: "exploration"
    references: ["Outer Wilds", "Chants of Sennaar"]
    extracted_at: "2024-01-22T10:05:00"

  mechanics:
    core_loop: "Observer â†’ DÃ©duire â†’ ExpÃ©rimenter"
    unique: "SystÃ¨me linguistique"
    extracted_at: "2024-01-22T10:15:00"

# DÃ©cisions importantes (pas toutes les rÃ©ponses)
decisions:
  - date: "2024-01-22T10:20:00"
    topic: "progression"
    decision: "Par la connaissance, pas XP"
    source: "party-mode-session-123"

  - date: "2024-01-22T11:00:00"
    topic: "art_style"
    decision: "StylisÃ© avec bioluminescence"
    source: "workflow-game-brief"
```

### 2. Extraction de Facts

Au lieu de stocker le texte brut des rÃ©ponses, extraire des **facts structurÃ©s** :

```python
# web-ui/server/services/fact_extractor.py

FACT_EXTRACTION_PROMPT = """
Extrais les informations clÃ©s de cette rÃ©ponse.

RÃ©ponse de l'utilisateur:
{user_response}

Question posÃ©e:
{question}

Contexte workflow:
{workflow_step}

Retourne un JSON avec les facts extraits:
{
  "facts": [
    {
      "category": "vision|mechanics|style|...",
      "key": "nom_court",
      "value": "valeur concise",
      "confidence": 0.0-1.0
    }
  ],
  "is_decision": true/false,
  "decision_summary": "si is_decision, rÃ©sumÃ© court"
}
"""

async def extract_facts(response: str, context: dict) -> dict:
    """
    Utilise un petit modÃ¨le (haiku) pour extraire les facts.
    CoÃ»t: ~100 tokens par extraction.
    Ã‰conomie: Ã©vite d'envoyer le texte brut (500+ tokens) Ã  chaque appel.
    """
    pass
```

### 3. Injection Contextuelle par Ã‰tape

Chaque Ã©tape de workflow dÃ©finit ce dont elle a besoin :

```yaml
# Dans workflow YAML
steps:
  - id: mechanics
    title: "Core Mechanics"

    # Ce qu'il faut injecter pour cette Ã©tape
    context_needs:
      facts:
        - vision.pitch        # Besoin du pitch
        - vision.fantasy      # Et du fantasy
        - genre.primary       # Et du genre
      decisions:
        - topic: progression  # Si dÃ©cision existe
      documents:
        - gdd.mechanics       # Section GDD si existe

    # Ce qu'il NE FAUT PAS injecter (implicite)
    # - Tout le GDD
    # - Historique complet
    # - Autres sessions
```

### 4. RÃ©sumÃ© Intelligent des Sessions

Pour les longues conversations, crÃ©er des rÃ©sumÃ©s progressifs :

```python
# web-ui/server/services/context_summarizer.py

SUMMARIZE_PROMPT = """
RÃ©sume cette conversation en conservant:
1. Les dÃ©cisions prises
2. Les facts importants
3. Les questions ouvertes

Conversation:
{messages}

Format:
{
  "summary": "rÃ©sumÃ© en 2-3 phrases",
  "key_points": ["point1", "point2"],
  "open_questions": ["question1"],
  "facts_to_keep": {...}
}
"""

async def should_summarize(session: dict) -> bool:
    """DÃ©cide si on doit rÃ©sumer (ex: > 10 Ã©changes)"""
    return len(session.get('messages', [])) > 10

async def summarize_and_compact(session: dict) -> dict:
    """
    1. Garde les 3 derniers messages intacts
    2. RÃ©sume le reste
    3. Extrait les facts
    4. RÃ©duit la taille de 80%+
    """
    pass
```

### 5. Cache de Contexte (Prompt Caching)

Utiliser le cache Anthropic pour les parties statiques :

```python
# web-ui/server/services/llm_service.py

class ContextCache:
    """
    Le prompt caching d'Anthropic permet de rÃ©utiliser
    le prÃ©fixe du prompt entre les appels.

    Structure optimale:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ CACHED (stable entre appels)    â”‚
    â”‚ - System prompt agent           â”‚
    â”‚ - Facts projet                  â”‚
    â”‚ - Contexte workflow             â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ NON-CACHED (change Ã  chaque)    â”‚
    â”‚ - Question courante             â”‚
    â”‚ - Derniers messages             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """

    def build_cached_context(self, project_id: str) -> str:
        """Partie stable du contexte (facts, agent)"""
        pass

    def build_dynamic_context(self, session: dict) -> str:
        """Partie qui change (messages rÃ©cents)"""
        pass
```

### 6. Interface Context Builder

```typescript
// web-ui/src/services/contextBuilder.ts

interface ContextLevel {
  level: 'core' | 'relevant' | 'detailed';
  tokens_estimate: number;
  content: string;
}

interface ContextRequest {
  workflow_id: string;
  step_id: string;
  include_history?: boolean;
  max_tokens?: number;  // Budget max
}

interface BuiltContext {
  levels: ContextLevel[];
  total_tokens: number;
  truncated: boolean;
  cache_key?: string;
}

async function buildContext(request: ContextRequest): Promise<BuiltContext> {
  // 1. Toujours inclure CORE
  // 2. Ajouter RELEVANT si dans le budget
  // 3. Ajouter DETAILED seulement si explicitement demandÃ©
}
```

### 7. Visualisation Debug (Dev Mode)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ” Context Debug                                [Dev Mode] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  ğŸ“Š Tokens envoyÃ©s: 2,340 / 100,000 max                    â”‚
â”‚     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 2.3%        â”‚
â”‚                                                             â”‚
â”‚  ğŸ“¦ Composition:                                           â”‚
â”‚     CORE      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  480 tokens (20%)               â”‚
â”‚     RELEVANT  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  1,200 tokens (51%)     â”‚
â”‚     MESSAGES  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  660 tokens (28%)               â”‚
â”‚                                                             â”‚
â”‚  ğŸ’¾ Cache hit: Yes (saved 480 tokens)                      â”‚
â”‚                                                             â”‚
â”‚  ğŸ“‹ Facts injectÃ©s:                                        â”‚
â”‚     â€¢ vision.pitch âœ“                                       â”‚
â”‚     â€¢ genre.primary âœ“                                      â”‚
â”‚     â€¢ mechanics.core_loop âœ“                                â”‚
â”‚                                                             â”‚
â”‚  âš ï¸ Truncated: historique (gardÃ© 5 derniers messages)     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Fichiers Ã  CrÃ©er

- `web-ui/server/services/context_manager.py` â€” Service principal
- `web-ui/server/services/fact_extractor.py` â€” Extraction de facts
- `web-ui/server/services/context_summarizer.py` â€” RÃ©sumÃ©s intelligents
- `web-ui/server/services/context_cache.py` â€” Gestion du cache
- `web-ui/src/services/contextBuilder.ts` â€” Client-side builder
- `web-ui/src/components/debug/ContextDebug.tsx` â€” Visualisation dev

## Fichiers Ã  Modifier

- `web-ui/server/services/llm_service.py` â€” IntÃ©grer le context manager
- `web-ui/server/services/workflow_service.py` â€” Utiliser context_needs
- `.unreal-companion/config.yaml` â€” Ajouter settings contexte

## Config Utilisateur

```yaml
# ~/.unreal-companion/config.yaml
context:
  # Budget tokens par dÃ©faut
  max_tokens: 4000

  # Extraction automatique de facts
  auto_extract_facts: true

  # RÃ©sumer aprÃ¨s N messages
  summarize_threshold: 10

  # Mode debug (affiche composition)
  debug_mode: false

  # Utiliser le cache Anthropic
  use_prompt_caching: true
```

## CritÃ¨res d'Acceptation

### RÃ©duction de Tokens

- [ ] Contexte moyen < 3000 tokens (vs 12000+ sans optimisation)
- [ ] Facts extraits automatiquement aprÃ¨s chaque rÃ©ponse
- [ ] RÃ©sumÃ© automatique aprÃ¨s 10+ messages
- [ ] Cache hit > 50% pour les parties stables

### QualitÃ© des RÃ©ponses

- [ ] Pas de perte d'information critique (facts prÃ©servÃ©s)
- [ ] CohÃ©rence maintenue sur les sessions longues
- [ ] Pas d'hallucination sur les dÃ©cisions passÃ©es
- [ ] Tests de rÃ©gression sur la qualitÃ©

### Performance

- [ ] Temps d'extraction facts < 500ms
- [ ] Temps de construction contexte < 100ms
- [ ] Cache invalidÃ© correctement sur changements

### ExpÃ©rience DÃ©veloppeur

- [ ] Mode debug avec visualisation tokens
- [ ] Logs clairs sur ce qui est injectÃ©
- [ ] Config simple pour ajuster les seuils

## Tests Ã  Ã‰crire

### Unit Tests

```python
def test_fact_extraction_from_response():
    # VÃ©rifie que les facts sont bien extraits

def test_context_level_building():
    # VÃ©rifie la construction par niveau

def test_summarization_quality():
    # VÃ©rifie que le rÃ©sumÃ© garde les infos clÃ©s

def test_cache_invalidation():
    # VÃ©rifie que le cache se vide quand il faut
```

### Integration Tests

```python
def test_workflow_with_optimized_context():
    # ExÃ©cute un workflow complet
    # VÃ©rifie que les tokens restent < budget

def test_long_session_summarization():
    # Simule une session de 20+ messages
    # VÃ©rifie la qualitÃ© aprÃ¨s rÃ©sumÃ©

def test_context_consistency_across_sessions():
    # Reprend une session aprÃ¨s pause
    # VÃ©rifie que les facts sont intacts
```

## Notes

### Anti-patterns Ã  Ã‰viter

- âŒ Envoyer tout le GDD Ã  chaque appel
- âŒ Garder l'historique complet en mÃ©moire
- âŒ Redemander des infos dÃ©jÃ  extraites
- âŒ RÃ©sumer trop tÃ´t (perte d'info)
- âŒ RÃ©sumer trop tard (budget explosÃ©)

### Patterns RecommandÃ©s

- âœ… Facts structurÃ©s > texte brut
- âœ… Injection sÃ©lective par Ã©tape
- âœ… RÃ©sumÃ© progressif (pas tout d'un coup)
- âœ… Cache des parties stables
- âœ… Budget tokens explicite

### Compromis QualitÃ©/CoÃ»t

| Mode | Tokens | CoÃ»t | QualitÃ© |
|------|--------|------|---------|
| **Minimal** | ~1500 | TrÃ¨s bas | Risque de perte |
| **Balanced** (dÃ©faut) | ~3000 | Bas | Bon |
| **Detailed** | ~6000 | Moyen | TrÃ¨s bon |
| **Full** | ~12000+ | Ã‰levÃ© | Maximum |

L'utilisateur peut choisir via config ou par session.
