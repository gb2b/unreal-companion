# P2.10 - LLM Performance & Architecture Refactoring

## Statut

- **Priorit√©** : Critical
- **Phase** : 2 - Studio Core
- **D√©pendances** : P2.0, P2.6, P2.7
- **Date cr√©ation** : 2026-01-23
- **Derni√®re r√©vision** : 2026-01-23 (Review Game Architect)

---

## R√©sum√© de la Review Architecturale

### Contexte

Review compl√®te du web-ui (frontend + backend) r√©alis√©e le 23/01/2026 avec l'approche Game Architect.

### Probl√®mes Critiques Identifi√©s

| # | Probl√®me | Impact | S√©v√©rit√© |
|---|----------|--------|----------|
| 1 | **Double appels LLM au d√©marrage workflow** | Latence 2x, co√ªt 2x | üî¥ Critical |
| 2 | **Pas de streaming depuis le premier token** | UX lente, impression de freeze | üî¥ Critical |
| 3 | **Contexte trop volumineux** | Tokens gaspill√©s, r√©ponses lentes | üî¥ Critical |
| 4 | **Pas de cache/parsing des documents** | Contexte recalcul√© √† chaque appel | üü† High |
| 5 | **i18n incompl√®te** | Strings hardcod√©es FR/EN m√©lang√©es | üü† High |
| 6 | **S√©lection de mod√®le non adaptative** | Mod√®le lourd pour t√¢ches simples | üü° Medium |
| 7 | **Deux syst√®mes workflow coexistants** | Code dupliqu√©, confusion | üü° Medium |

### Ce Qui Fonctionne

- Architecture Zustand stores bien structur√©e
- Services backend bien organis√©s
- Syst√®me de workflow YAML extensible
- Multi-providers LLM fonctionnel
- i18n base en place (juste incompl√®te)

---

## Objectif

Optimiser l'architecture LLM pour :
1. **R√©duire la latence** : R√©ponse visible en < 500ms
2. **R√©duire les co√ªts** : Moins de tokens, mod√®les adapt√©s
3. **Am√©liorer l'UX** : Streaming, feedback visuel, r√©activit√©

---

## Sous-Tasks (R√©vis√©es)

### Vue d'Ensemble

| # | Sous-task | Priorit√© | Complexit√© | Phase |
|---|-----------|----------|------------|-------|
| 0 | Instrumentation & Baseline | üî¥ Critical | Basse | P2 |
| A | Nettoyage syst√®me dual workflow | üî¥ Critical | Moyenne | P2 |
| B | √âliminer les double appels LLM | üî¥ Critical | Moyenne | P2 |
| C | Streaming & Feedback Visuel | üî¥ Critical | Moyenne | P2 |
| D | Knowledge Pipeline (parsing/cache) | üü† High | Haute | P2 |
| E | Compl√©ter i18n | üü° Medium | Basse | P2 |
| F | Context Enrichment Lite | üü° Medium | Moyenne | **P5** |
| G | S√©lection de mod√®le adaptative | üü° Medium | Moyenne | **P5** |

### Graphe de D√©pendances Corrig√©

```
[0] Instrumentation
         ‚îÇ
         ‚ñº
[A] Nettoyage Dual ‚îÄ‚îÄ‚Üí [B] Single Call ‚îÄ‚îÄ‚Üí [C] Streaming + Thinking
         ‚îÇ                    ‚îÇ
         ‚îÇ                    ‚îî‚îÄ‚îÄ‚Üí [D] Knowledge Pipeline ‚îÄ‚îÄ‚Üí [E] i18n
         ‚îÇ                                   ‚îÇ
         ‚îÇ                                   ‚îî‚îÄ‚îÄ‚Üí [F] Context Enrichment (P5)
         ‚îÇ                                   ‚îî‚îÄ‚îÄ‚Üí [G] Model Router (P5)
         ‚îÇ
         ‚îî‚îÄ‚îÄ (d√©bloque tout le reste)
```

---

## Sous-Task 0 : Instrumentation & Baseline

### Objectif

Mesurer l'√©tat actuel AVANT toute modification pour pouvoir quantifier les am√©liorations.

### M√©triques √† Capturer

| M√©trique | Comment mesurer | O√π instrumenter |
|----------|-----------------|-----------------|
| Time to First Token (TTFT) | `Date.now()` start ‚Üí premier chunk SSE | Frontend + Backend |
| Time to Complete (TTC) | Start ‚Üí r√©ponse compl√®te | Frontend |
| Tokens envoy√©s | Compter dans le prompt | `llm.py` |
| Tokens re√ßus | Compter dans la r√©ponse | `llm.py` |
| Nombre d'appels LLM | Counter par session | `llm.py` |
| Taille contexte | Len(context_string) | `context_builder.py` |

### Fichiers √† cr√©er/modifier

| Fichier | Action |
|---------|--------|
| `server/services/metrics.py` | **Cr√©er** - Service de m√©triques simple |
| `server/services/llm.py` | Ajouter logging des m√©triques |
| `web-ui/src/hooks/useMetrics.ts` | **Cr√©er** - Hook pour mesurer c√¥t√© client |

### Sp√©cifications Techniques

```python
# server/services/metrics.py

import time
import logging
from dataclasses import dataclass, field
from typing import Optional

logger = logging.getLogger("metrics")

@dataclass
class LLMCallMetrics:
    """M√©triques d'un appel LLM"""
    session_id: str
    call_id: str
    start_time: float = field(default_factory=time.time)

    # Timing
    time_to_first_token: Optional[float] = None
    time_to_complete: Optional[float] = None

    # Tokens
    input_tokens: int = 0
    output_tokens: int = 0
    context_size_chars: int = 0

    # Meta
    provider: str = ""
    model: str = ""
    task_type: str = ""  # extraction, conversation, generation

    def mark_first_token(self):
        self.time_to_first_token = time.time() - self.start_time

    def mark_complete(self, input_tokens: int, output_tokens: int):
        self.time_to_complete = time.time() - self.start_time
        self.input_tokens = input_tokens
        self.output_tokens = output_tokens
        self._log()

    def _log(self):
        logger.info(
            f"LLM_CALL | session={self.session_id} | "
            f"ttft={self.time_to_first_token:.2f}s | "
            f"ttc={self.time_to_complete:.2f}s | "
            f"in={self.input_tokens} | out={self.output_tokens} | "
            f"ctx={self.context_size_chars} | "
            f"provider={self.provider} | model={self.model}"
        )

class MetricsCollector:
    """Collecteur de m√©triques (singleton)"""

    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.calls = []
        return cls._instance

    def new_call(self, session_id: str, **kwargs) -> LLMCallMetrics:
        call = LLMCallMetrics(
            session_id=session_id,
            call_id=f"{session_id}_{len(self.calls)}",
            **kwargs
        )
        self.calls.append(call)
        return call

    def get_baseline_report(self) -> dict:
        """G√©n√®re un rapport des m√©triques actuelles"""
        if not self.calls:
            return {"error": "No calls recorded"}

        ttfts = [c.time_to_first_token for c in self.calls if c.time_to_first_token]
        ttcs = [c.time_to_complete for c in self.calls if c.time_to_complete]
        inputs = [c.input_tokens for c in self.calls]

        return {
            "total_calls": len(self.calls),
            "avg_ttft": sum(ttfts) / len(ttfts) if ttfts else 0,
            "avg_ttc": sum(ttcs) / len(ttcs) if ttcs else 0,
            "avg_input_tokens": sum(inputs) / len(inputs) if inputs else 0,
            "calls_per_session": self._calls_per_session(),
        }

    def _calls_per_session(self) -> float:
        sessions = set(c.session_id for c in self.calls)
        return len(self.calls) / len(sessions) if sessions else 0
```

### Crit√®res d'acceptation

- [ ] M√©triques logg√©es pour chaque appel LLM
- [ ] Rapport baseline g√©n√©rable via endpoint `/api/metrics/baseline`
- [ ] Mesures c√¥t√© frontend (TTFT per√ßu)
- [ ] Pas d'impact sur les performances (< 1ms overhead)

### Dur√©e estim√©e : 0.5 jour

---

## Sous-Task A : Nettoyage Syst√®me Dual Workflow

> **Note** : Cette task doit √™tre faite EN PREMIER pour √©viter d'impl√©menter le streaming dans deux syst√®mes.

### Probl√®me

Deux syst√®mes coexistent :
1. **Chat WebSocket** : Pour conversation libre (dans `workflowStore.ts`)
2. **Step REST** : Pour workflows structur√©s

Code dupliqu√©, confusion sur lequel utiliser quand.

### Solution

Clarifier et s√©parer proprement :
- **Mode Workflow** = Toujours step-based (REST uniquement)
- **Mode Chat** = Conversation libre (SSE pour streaming)
- **Transition** : Chat peut proposer de lancer un workflow ‚Üí switch de mode

### Architecture Cible

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        StudioPage                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚ñº                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  workflowStore  ‚îÇ       ‚îÇ    chatStore    ‚îÇ
‚îÇ  (step-based)   ‚îÇ       ‚îÇ  (free-form)    ‚îÇ
‚îÇ                 ‚îÇ       ‚îÇ                 ‚îÇ
‚îÇ - REST API      ‚îÇ       ‚îÇ - SSE stream    ‚îÇ
‚îÇ - Session steps ‚îÇ       ‚îÇ - Messages[]    ‚îÇ
‚îÇ - No WebSocket  ‚îÇ       ‚îÇ - Agent context ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                         ‚îÇ
         ‚ñº                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ WorkflowStep    ‚îÇ       ‚îÇ ChatPanel       ‚îÇ
‚îÇ Container       ‚îÇ       ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fichiers √† modifier

| Fichier | Modification |
|---------|--------------|
| `web-ui/src/stores/workflowStore.ts` | Supprimer WebSocket, garder REST uniquement |
| `web-ui/src/stores/chatStore.ts` | **Cr√©er** - Store d√©di√© au chat conversationnel |
| `web-ui/src/components/chat/*` | Migrer vers `chatStore` |
| `web-ui/src/components/workflow/*` | Confirmer utilisation `workflowStore` |

### Sp√©cifications Techniques

```typescript
// web-ui/src/stores/chatStore.ts (NOUVEAU)

import { create } from 'zustand'

interface ChatMessage {
  id: string
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
  isStreaming?: boolean
}

interface ChatState {
  // State
  messages: ChatMessage[]
  isStreaming: boolean
  currentAgentId: string | null

  // Actions
  sendMessage: (content: string) => Promise<void>
  clearChat: () => void
  setAgent: (agentId: string) => void

  // Workflow trigger
  triggerWorkflow: (workflowId: string) => void
}

export const useChatStore = create<ChatState>((set, get) => ({
  messages: [],
  isStreaming: false,
  currentAgentId: null,

  sendMessage: async (content: string) => {
    const { currentAgentId, messages } = get()

    // Add user message
    const userMsg: ChatMessage = {
      id: crypto.randomUUID(),
      role: 'user',
      content,
      timestamp: new Date()
    }
    set({ messages: [...messages, userMsg], isStreaming: true })

    // Stream response via SSE
    const response = await fetch('/api/chat/stream', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        message: content,
        agent_id: currentAgentId,
        history: messages.slice(-10) // Last 10 messages for context
      })
    })

    // Handle SSE stream...
    // (voir Sous-Task C pour l'impl√©mentation)
  },

  triggerWorkflow: (workflowId: string) => {
    // Import workflowStore and start workflow
    // This creates a clean transition
    const { startWorkflow } = useWorkflowStore.getState()
    startWorkflow(workflowId, get().currentAgentId)
  },

  // ...
}))
```

```typescript
// web-ui/src/stores/workflowStore.ts (SIMPLIFI√â)

// SUPPRIMER :
// - WebSocket connection
// - sendChatMessage()
// - chatMessages state

// GARDER :
// - activeSession
// - currentStepData
// - startWorkflow()
// - submitStepResponse()
// - fetchCurrentStep()
```

### Crit√®res d'acceptation

- [ ] `workflowStore` n'a plus de WebSocket
- [ ] `chatStore` cr√©√© et fonctionnel
- [ ] Chat et Workflow sont deux modes distincts
- [ ] Transition Chat ‚Üí Workflow fonctionne
- [ ] Tests existants passent toujours

### Dur√©e estim√©e : 1.5 jours

---

## Sous-Task B : √âliminer les Double Appels LLM

### Probl√®me

```
Flux actuel (MAUVAIS) :
1. User clique "Start Workflow"
2. POST /api/workflows/start ‚Üí Appel LLM #1 (intro message)
3. Frontend re√ßoit session_id
4. GET /api/workflows/{id}/step ‚Üí Appel LLM #2 (premier step)
   ‚Ü≥ PROBL√àME : 2 appels LLM pour afficher le premier step !
```

### Solution

```
Flux corrig√© (BON) :
1. User clique "Start Workflow"
2. POST /api/workflows/start ‚Üí Appel LLM unique (intro + premier step)
3. Frontend re√ßoit session_id + step_data
4. Affichage imm√©diat (pas de second appel)
```

### Fichiers √† modifier

| Fichier | Modification |
|---------|--------------|
| `server/api/workflows.py` | `/start` retourne aussi `first_step_data` |
| `server/services/workflow/engine.py` | `start_workflow()` retourne le step rendu |
| `web-ui/src/stores/workflowStore.ts` | Utiliser `first_step_data` directement |
| `web-ui/src/components/workflow/WorkflowStepContainer.tsx` | Ne pas appeler `/step` si data d√©j√† pr√©sente |

### Sp√©cifications Techniques

```python
# server/api/workflows.py

@router.post("/start")
async def start_workflow(request: StartWorkflowRequest):
    # Cr√©er la session
    session = await workflow_engine.start_session(
        workflow_id=request.workflow_id,
        project_id=request.project_id,
        agent_id=request.agent_id
    )

    # NOUVEAU : Rendre le premier step imm√©diatement
    first_step = await workflow_engine.render_current_step(session.id)

    return {
        "session_id": session.id,
        "workflow_id": session.workflow_id,
        "current_step": session.current_step,
        "first_step_data": first_step,  # AJOUT
        "total_steps": session.total_steps,
        "status": "active"
    }
```

```typescript
// web-ui/src/stores/workflowStore.ts

startWorkflow: async (workflowId, agentId) => {
  set({ isLoading: true, error: null })

  try {
    const response = await api.post('/api/workflows/start', {
      workflow_id: workflowId,
      agent_id: agentId,
      project_id: currentProject?.id
    })

    set({
      activeSession: response,
      // NOUVEAU : Utiliser directement les donn√©es du step
      currentStepData: response.first_step_data,
      isLoading: false
    })

    // PAS de fetchCurrentStep() ici !
  } catch (error) {
    set({ error: error.message, isLoading: false })
  }
}
```

```typescript
// web-ui/src/components/workflow/WorkflowStepContainer.tsx

useEffect(() => {
  // Ne fetch QUE si on n'a pas d√©j√† les donn√©es
  if (activeSession && !currentStepData) {
    fetchCurrentStep(activeSession.session_id)
  }
}, [activeSession?.session_id])

// Supprimer le fetch syst√©matique au mount
```

### Crit√®res d'acceptation

- [ ] Un seul appel LLM au d√©marrage d'un workflow
- [ ] Le premier step s'affiche en < 2s (hors latence LLM)
- [ ] M√©triques montrent 1 call au lieu de 2
- [ ] Les workflows existants continuent de fonctionner
- [ ] Tests unitaires passent

### Dur√©e estim√©e : 2 jours

---

## Sous-Task C : Streaming & Feedback Visuel

> **Note** : Fusion des anciennes Sous-Tasks B (Streaming) et E (Extended Thinking)

### Probl√®me

1. Le backend supporte le streaming mais le frontend attend la r√©ponse compl√®te
2. Pendant la g√©n√©ration, l'utilisateur voit juste un spinner

### Solution

1. Impl√©menter SSE (Server-Sent Events) pour afficher les tokens progressivement
2. Afficher un indicateur "thinking" pendant la g√©n√©ration

### Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     SSE Stream      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   LLM API   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ   Backend   ‚îÇ
‚îÇ  (stream)   ‚îÇ  token par token    ‚îÇ   FastAPI   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                    ‚îÇ                      ‚îÇ
                    ‚ñº                      ‚ñº
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ   thinking   ‚îÇ       ‚îÇ    tokens    ‚îÇ
           ‚îÇ   events     ‚îÇ       ‚îÇ    events    ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ                      ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  StreamingResponse  ‚îÇ
                    ‚îÇ     Component       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Design UI

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ü§î L'agent r√©fl√©chit...                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Analyse du brief...                             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Consultation de la section gameplay du GDD...   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Formulation des questions...                    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ R√©ponse ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ           ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Bas√© sur ton brief, je te propose...                  ‚îÇ
‚îÇ  [texte qui s'affiche progressivement]‚ñä                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fichiers √† cr√©er/modifier

| Fichier | Action |
|---------|--------|
| `server/api/workflows.py` | Endpoint SSE `/stream/{session_id}` |
| `server/api/chat.py` | Endpoint SSE `/chat/stream` |
| `server/services/llm.py` | Exposer le g√©n√©rateur de stream avec thinking |
| `web-ui/src/hooks/useStreamingResponse.ts` | **Cr√©er** |
| `web-ui/src/components/common/ThinkingIndicator.tsx` | **Cr√©er** |
| `web-ui/src/components/common/StreamingText.tsx` | **Cr√©er** |

### Sp√©cifications Techniques

```python
# server/api/workflows.py

from fastapi.responses import StreamingResponse
from enum import Enum

class StreamEventType(str, Enum):
    THINKING = "thinking"
    TOKEN = "token"
    COMPLETE = "complete"
    ERROR = "error"

@router.get("/stream/{session_id}")
async def stream_step_response(session_id: str):
    """SSE endpoint pour streaming des r√©ponses LLM"""

    async def event_generator():
        try:
            async for chunk in workflow_engine.stream_step(session_id):
                event_data = json.dumps({
                    'type': chunk.type,
                    'content': chunk.content,
                    'data': chunk.data if hasattr(chunk, 'data') else None
                })
                yield f"data: {event_data}\n\n"

            yield "data: [DONE]\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'content': str(e)})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        }
    )
```

```python
# server/services/llm.py - M√©thode stream enrichie

async def stream_with_thinking(
    self,
    prompt: str,
    system: str = None,
    **kwargs
) -> AsyncGenerator[StreamChunk, None]:
    """Stream avec extraction du thinking"""

    # Ajouter instruction pour le thinking
    thinking_prompt = """
    Avant de r√©pondre, indique bri√®vement (en 2-3 points) ce que tu analyses.
    Format:
    <thinking>
    ‚Ä¢ Point 1
    ‚Ä¢ Point 2
    </thinking>

    Puis donne ta r√©ponse.
    """

    full_prompt = thinking_prompt + "\n\n" + prompt

    in_thinking = False
    thinking_buffer = ""

    async for token in self._raw_stream(full_prompt, system, **kwargs):
        # D√©tecter les balises thinking
        if "<thinking>" in token:
            in_thinking = True
            continue
        if "</thinking>" in token:
            in_thinking = False
            # √âmettre les thoughts accumul√©s
            for line in thinking_buffer.strip().split('\n'):
                if line.strip():
                    yield StreamChunk(type="thinking", content=line.strip())
            thinking_buffer = ""
            continue

        if in_thinking:
            thinking_buffer += token
        else:
            yield StreamChunk(type="token", content=token)
```

```typescript
// web-ui/src/hooks/useStreamingResponse.ts

import { useState, useCallback, useRef } from 'react'

interface StreamState {
  text: string
  thoughts: string[]
  isStreaming: boolean
  isComplete: boolean
  error: string | null
}

export function useStreamingResponse() {
  const [state, setState] = useState<StreamState>({
    text: '',
    thoughts: [],
    isStreaming: false,
    isComplete: false,
    error: null
  })

  const eventSourceRef = useRef<EventSource | null>(null)

  const startStream = useCallback((url: string) => {
    // Cleanup previous
    if (eventSourceRef.current) {
      eventSourceRef.current.close()
    }

    setState({
      text: '',
      thoughts: [],
      isStreaming: true,
      isComplete: false,
      error: null
    })

    const eventSource = new EventSource(url)
    eventSourceRef.current = eventSource

    eventSource.onmessage = (event) => {
      if (event.data === '[DONE]') {
        eventSource.close()
        setState(prev => ({ ...prev, isStreaming: false, isComplete: true }))
        return
      }

      try {
        const data = JSON.parse(event.data)

        switch (data.type) {
          case 'thinking':
            setState(prev => ({
              ...prev,
              thoughts: [...prev.thoughts, data.content]
            }))
            break
          case 'token':
            setState(prev => ({
              ...prev,
              text: prev.text + data.content
            }))
            break
          case 'error':
            setState(prev => ({
              ...prev,
              error: data.content,
              isStreaming: false
            }))
            break
          case 'complete':
            // Donn√©es structur√©es finales (pour workflows)
            setState(prev => ({
              ...prev,
              isComplete: true
            }))
            break
        }
      } catch (e) {
        console.error('Failed to parse SSE event:', e)
      }
    }

    eventSource.onerror = () => {
      eventSource.close()
      setState(prev => ({
        ...prev,
        isStreaming: false,
        error: 'Connection lost'
      }))
    }

    return () => {
      eventSource.close()
    }
  }, [])

  const reset = useCallback(() => {
    if (eventSourceRef.current) {
      eventSourceRef.current.close()
    }
    setState({
      text: '',
      thoughts: [],
      isStreaming: false,
      isComplete: false,
      error: null
    })
  }, [])

  return { ...state, startStream, reset }
}
```

```typescript
// web-ui/src/components/common/ThinkingIndicator.tsx

import { Brain } from 'lucide-react'
import { cn } from '@/lib/utils'
import { useTranslation } from '@/i18n/useI18n'

interface ThinkingIndicatorProps {
  thoughts: string[]
  isActive: boolean
  className?: string
}

export function ThinkingIndicator({
  thoughts,
  isActive,
  className
}: ThinkingIndicatorProps) {
  const { t } = useTranslation()

  if (thoughts.length === 0 && !isActive) return null

  return (
    <div className={cn(
      "rounded-lg bg-muted/50 p-4 mb-4 transition-all duration-300",
      isActive ? "opacity-100" : "opacity-60 scale-95",
      className
    )}>
      <div className="flex items-center gap-2 mb-2 text-sm text-muted-foreground">
        <Brain className={cn("h-4 w-4", isActive && "animate-pulse")} />
        <span>
          {isActive
            ? t('streaming.thinking')
            : t('streaming.thinkingDone')}
        </span>
      </div>

      <ul className="space-y-1 text-sm">
        {thoughts.map((thought, i) => (
          <li key={i} className="flex items-start gap-2">
            <span className="text-muted-foreground mt-0.5">‚Ä¢</span>
            <span className={cn(
              "transition-opacity duration-200",
              i === thoughts.length - 1 && isActive && "animate-pulse"
            )}>
              {thought}
            </span>
          </li>
        ))}
      </ul>
    </div>
  )
}
```

```typescript
// web-ui/src/components/common/StreamingText.tsx

import ReactMarkdown from 'react-markdown'
import { cn } from '@/lib/utils'

interface StreamingTextProps {
  text: string
  isStreaming: boolean
  className?: string
}

export function StreamingText({ text, isStreaming, className }: StreamingTextProps) {
  return (
    <div className={cn("prose prose-sm dark:prose-invert", className)}>
      <ReactMarkdown>{text}</ReactMarkdown>
      {isStreaming && (
        <span className="inline-block w-2 h-4 bg-foreground animate-pulse ml-0.5">
          ‚ñä
        </span>
      )}
    </div>
  )
}
```

### Crit√®res d'acceptation

- [ ] Premier token visible en < 500ms apr√®s l'appel LLM
- [ ] Texte s'affiche progressivement (effet "typing")
- [ ] Thinking visible pendant le streaming
- [ ] Animation fluide (pas de saccades)
- [ ] Fonctionne avec Anthropic, OpenAI, Ollama
- [ ] Fallback gracieux si streaming non support√©
- [ ] Pas de memory leak (EventSource bien ferm√©)

### Dur√©e estim√©e : 3 jours

---

## Sous-Task D : Knowledge Pipeline

### Probl√®me

√Ä chaque appel LLM, on envoie les documents entiers dans le contexte :
- Brief complet (peut faire 5000+ tokens)
- GDD complet (peut faire 20000+ tokens)
- Historique de conversation

### Solution

Cr√©er un pipeline de "parsing intelligent" qui :
1. **Parse** les documents une fois (√† la cr√©ation/modification)
2. **Extrait** les faits cl√©s (structured data)
3. **Cache** ces extractions
4. **Utilise** un contexte minimal pour le LLM

### Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Document cr√©√©  ‚îÇ
‚îÇ  (Brief, GDD)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DocumentParser ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  FactsExtractor  ‚îÇ
‚îÇ  (par type)     ‚îÇ     ‚îÇ  (LLM l√©ger)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚ñº
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ   Facts Cache    ‚îÇ
                        ‚îÇ   (SQLite)       ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Context Builder                   ‚îÇ
‚îÇ  brief_facts + gdd_facts + current_step     ‚îÇ
‚îÇ  = prompt compact (~2000 tokens max)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fichiers √† cr√©er

| Fichier | Description |
|---------|-------------|
| `server/services/knowledge/__init__.py` | Module knowledge |
| `server/services/knowledge/parser.py` | Parse documents par type |
| `server/services/knowledge/extractor.py` | Extrait les faits cl√©s |
| `server/services/knowledge/cache.py` | Cache SQLite des facts |
| `server/services/knowledge/context_builder.py` | Construit le contexte minimal |

### Sp√©cifications Techniques

```python
# server/services/knowledge/extractor.py

from dataclasses import dataclass
from datetime import datetime
from typing import Any
import hashlib

@dataclass
class DocumentFacts:
    """Faits extraits d'un document"""
    document_id: str
    document_type: str  # brief, gdd, architecture
    extracted_at: datetime
    source_hash: str  # Pour invalidation

    # Faits structur√©s
    facts: dict[str, Any]

    # R√©sum√© court (pour contexte minimal)
    summary: str  # ~200 tokens max

    # Sections disponibles (pour enrichment)
    sections: list[str]

class FactsExtractor:
    """Extrait les faits cl√©s d'un document"""

    def __init__(self, llm_service, model: str = "haiku"):
        self.llm = llm_service
        self.model = model  # Utiliser un mod√®le l√©ger/cheap

    async def extract(self, document) -> DocumentFacts:
        """Extrait les faits selon le type de document"""
        source_hash = hashlib.md5(document.content.encode()).hexdigest()

        if document.type == "brief":
            return await self._extract_brief(document, source_hash)
        elif document.type == "gdd":
            return await self._extract_gdd(document, source_hash)
        elif document.type == "architecture":
            return await self._extract_architecture(document, source_hash)
        else:
            return await self._extract_generic(document, source_hash)

    async def _extract_brief(self, doc, source_hash: str) -> DocumentFacts:
        """Extrait les faits cl√©s d'un Game Brief"""

        prompt = """Extrais les faits cl√©s de ce Game Brief.

Retourne un JSON avec cette structure exacte:
{
  "game_name": "nom du jeu",
  "genre": "genre principal",
  "target_audience": "audience cible",
  "core_loop": "description courte de la boucle de jeu",
  "unique_selling_points": ["USP1", "USP2"],
  "platforms": ["platform1"],
  "key_features": ["feature1", "feature2"],
  "summary": "R√©sum√© en 2-3 phrases du concept"
}

Document:
"""

        result = await self.llm.complete(
            prompt + doc.content,
            model=self.model,
            response_format="json"
        )

        facts = json.loads(result)

        return DocumentFacts(
            document_id=doc.id,
            document_type="brief",
            extracted_at=datetime.now(),
            source_hash=source_hash,
            facts=facts,
            summary=facts.get("summary", ""),
            sections=list(facts.keys())
        )
```

```python
# server/services/knowledge/context_builder.py

class ContextBuilder:
    """Construit un contexte minimal pour le LLM"""

    def __init__(self, cache, max_tokens: int = 2000):
        self.cache = cache
        self.MAX_TOKENS = max_tokens
        # Approximation : 1 token ‚âà 4 caract√®res
        self.MAX_CHARS = max_tokens * 4

    async def build_context(
        self,
        project_id: str,
        focus: str = None,  # "gameplay", "narrative", "technical"
        step_context: str = None  # Info sur le step actuel
    ) -> str:
        """Retourne un contexte compact pour le prompt"""

        # R√©cup√©rer les facts cach√©s
        brief_facts = await self.cache.get(project_id, "brief")
        gdd_facts = await self.cache.get(project_id, "gdd")

        context_parts = []

        # 1. Toujours inclure le summary du brief
        if brief_facts:
            context_parts.append(self._format_brief_context(brief_facts))

        # 2. GDD : filtrer par focus si sp√©cifi√©
        if gdd_facts:
            if focus:
                relevant = self._filter_by_focus(gdd_facts, focus)
                context_parts.append(f"## Relevant GDD ({focus})\n{relevant}")
            else:
                # Juste le summary
                context_parts.append(f"## GDD Summary\n{gdd_facts.summary}")

        # 3. Step context
        if step_context:
            context_parts.append(f"## Current Step\n{step_context}")

        # Assembler
        context = "\n\n".join(context_parts)

        # V√©rifier la taille et tronquer si n√©cessaire
        if len(context) > self.MAX_CHARS:
            context = self._truncate_intelligently(context)

        return context

    def _format_brief_context(self, facts: DocumentFacts) -> str:
        """Formate le brief de mani√®re compacte"""
        f = facts.facts
        return f"""## Game Brief
**{f.get('game_name', 'Untitled')}** - {f.get('genre', 'Unknown genre')}
Target: {f.get('target_audience', 'N/A')}
Core Loop: {f.get('core_loop', 'N/A')}
USPs: {', '.join(f.get('unique_selling_points', []))}"""

    def _filter_by_focus(self, facts: DocumentFacts, focus: str) -> str:
        """Filtre les sections GDD par focus"""
        focus_mapping = {
            "gameplay": ["mechanics", "systems", "progression", "combat"],
            "narrative": ["story", "characters", "world", "lore"],
            "technical": ["architecture", "performance", "platforms", "tools"]
        }

        relevant_keys = focus_mapping.get(focus, [])
        relevant_facts = {
            k: v for k, v in facts.facts.items()
            if any(rk in k.lower() for rk in relevant_keys)
        }

        return json.dumps(relevant_facts, indent=2)
```

### Schema de Cache

```sql
-- Table pour les facts extraits
CREATE TABLE IF NOT EXISTS document_facts (
    id TEXT PRIMARY KEY,
    project_id TEXT NOT NULL,
    document_id TEXT NOT NULL,
    document_type TEXT NOT NULL,
    extracted_at TIMESTAMP NOT NULL,
    facts_json TEXT NOT NULL,
    summary TEXT NOT NULL,
    sections_json TEXT,
    source_hash TEXT NOT NULL,

    UNIQUE(project_id, document_type)
);

CREATE INDEX IF NOT EXISTS idx_facts_project ON document_facts(project_id);
```

### Trigger d'extraction

```python
# server/api/documents.py

@router.post("/")
async def create_document(request: CreateDocumentRequest):
    # Cr√©er le document
    document = await documents_service.create(request)

    # Extraire les facts en background
    background_tasks.add_task(
        knowledge_service.extract_and_cache,
        document
    )

    return document

@router.put("/{document_id}")
async def update_document(document_id: str, request: UpdateDocumentRequest):
    # Mettre √† jour
    document = await documents_service.update(document_id, request)

    # Re-extraire si le contenu a chang√©
    if request.content:
        background_tasks.add_task(
            knowledge_service.extract_and_cache,
            document
        )

    return document
```

### Crit√®res d'acceptation

- [ ] Documents pars√©s une seule fois (√† la cr√©ation/modification)
- [ ] Facts cach√©s en SQLite
- [ ] Cache invalid√© si document modifi√© (hash check)
- [ ] Contexte LLM < 2000 tokens (configurable)
- [ ] Extraction fonctionne pour brief, gdd, architecture
- [ ] M√©triques montrent r√©duction des tokens
- [ ] Tests unitaires pour chaque extractor

### Dur√©e estim√©e : 4 jours

---

## Sous-Task E : Compl√©ter i18n

### Probl√®me

Strings hardcod√©es en fran√ßais et anglais m√©lang√©es dans les composants.

### Exemples de Probl√®mes

```typescript
// WorkflowStepContainer.tsx - MAUVAIS
<span>D√©marrage...</span>
<span>Pr√©paration du workflow...</span>

// Devrait √™tre :
<span>{t('workflow.starting')}</span>
<span>{t('workflow.preparing')}</span>
```

### Solution

1. Audit des strings hardcod√©es
2. Ajouter les cl√©s dans `translations.ts`
3. Remplacer par `t('key')`

### Fichiers √† auditer (prioritaires)

- `WorkflowStepContainer.tsx`
- `WorkflowStepView.tsx`
- `SuggestionCards.tsx`
- `AgentChatPanel.tsx`
- `ThinkingIndicator.tsx` (nouveau)
- `StreamingText.tsx` (nouveau)

### Cl√©s √† ajouter

```typescript
// √Ä ajouter dans translations.ts

// Section streaming (nouvelle)
'streaming.thinking': 'The agent is thinking...',
'streaming.thinkingDone': 'Analysis complete',
'streaming.error': 'An error occurred',
'streaming.connectionLost': 'Connection lost',

// Section workflow
'workflow.starting': 'Starting...',
'workflow.preparing': 'Preparing workflow...',
'workflow.loading': 'Loading step...',
'workflow.submitting': 'Submitting...',
'workflow.error': 'An error occurred',

// Versions FR
'streaming.thinking': "L'agent r√©fl√©chit...",
'streaming.thinkingDone': 'Analyse termin√©e',
// etc.
```

### Crit√®res d'acceptation

- [ ] Aucune string UI hardcod√©e dans les composants modifi√©s
- [ ] Toutes les nouvelles cl√©s existent en EN, FR, ES
- [ ] Langue par d√©faut respect√©e au chargement
- [ ] Pas de r√©gression sur les traductions existantes

### Dur√©e estim√©e : 2 jours

---

## Sous-Task F : Context Enrichment Lite (Phase 5)

> **Note** : D√©plac√©e en Phase 5. Impl√©mentation "lite" sans tool_use complet.

### Probl√®me

Parfois le LLM a besoin de d√©tails sp√©cifiques qu'on ne peut pas anticiper avec le Knowledge Pipeline.

### Approche "Lite" (Sans Tool Use)

Au lieu d'utiliser le tool_use dynamique (qui n√©cessite un support provider), on utilise une approche **pr√©dictive** :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Approche Tool Use (Complexe)               ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  LLM demande ‚Üí Tool call ‚Üí Fetch ‚Üí Inject ‚Üí LLM continue   ‚îÇ
‚îÇ  (2+ round trips, d√©pend du provider)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                           VS

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Approche Lite (Simple)                     ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Step d√©finit ‚Üí Pre-fetch ‚Üí Inject ‚Üí LLM r√©pond            ‚îÇ
‚îÇ  (1 round trip, provider agnostic)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Solution Lite

1. Chaque step du workflow d√©clare les sections potentiellement utiles
2. Le backend pr√©-charge ces sections AVANT l'appel LLM
3. Pas besoin de tool_use, fonctionne avec tous les providers

### Sp√©cifications

```yaml
# Dans un workflow YAML
steps:
  - id: "gameplay_questions"
    name: "Gameplay Design"
    # NOUVEAU : sections √† pr√©-charger
    context_sections:
      - type: "gdd"
        sections: ["mechanics", "progression"]
      - type: "brief"
        sections: ["core_loop", "key_features"]
```

```python
# server/services/workflow/engine.py

async def render_step_with_context(self, session_id: str, step: WorkflowStep):
    """Rend un step avec contexte enrichi"""

    # 1. Contexte de base (facts)
    base_context = await self.context_builder.build_context(
        project_id=session.project_id,
        focus=step.focus
    )

    # 2. Enrichissement si d√©clar√© dans le step
    if step.context_sections:
        enriched = await self._prefetch_sections(
            session.project_id,
            step.context_sections
        )
        base_context += f"\n\n## Additional Context\n{enriched}"

    # 3. Appel LLM avec contexte enrichi
    return await self.llm.complete(
        prompt=step.prompt,
        system=base_context
    )
```

### Support Tool Use (Optionnel)

Pour les providers qui supportent tool_use, on peut ajouter un mode "dynamic" :

```python
# server/services/knowledge/dynamic_fetcher.py

class DynamicContextFetcher:
    """Fetch dynamique via tool_use (si support√©)"""

    TOOL_DEFINITION = {
        "name": "get_document_section",
        "description": "Get a specific section from a project document",
        "input_schema": {
            "type": "object",
            "properties": {
                "document_type": {
                    "type": "string",
                    "enum": ["brief", "gdd", "architecture"]
                },
                "section": {
                    "type": "string",
                    "description": "Section name (e.g., 'combat', 'progression')"
                }
            },
            "required": ["document_type", "section"]
        }
    }

    def is_supported(self, provider: str) -> bool:
        """V√©rifie si le provider supporte tool_use"""
        return provider in ["anthropic", "openai", "google"]

    async def handle_tool_call(
        self,
        project_id: str,
        tool_input: dict
    ) -> str:
        """Ex√©cute le tool call et retourne le contenu"""
        doc_type = tool_input["document_type"]
        section = tool_input.get("section")

        facts = await self.cache.get(project_id, doc_type)

        if section and section in facts.facts:
            return json.dumps(facts.facts[section])
        else:
            return facts.summary
```

### Crit√®res d'acceptation (Phase 5)

- [ ] Mode "lite" fonctionne avec tous les providers
- [ ] Steps peuvent d√©clarer leurs sections de contexte
- [ ] Mode "dynamic" disponible pour providers compatibles
- [ ] Limite de 3 tool calls max (√©viter boucles)
- [ ] Fallback automatique lite ‚Üí dynamic selon provider

### Dur√©e estim√©e : 3 jours

---

## Sous-Task G : S√©lection de Mod√®le Adaptative (Phase 5)

> **Note** : D√©plac√©e en Phase 5. N√©cessite d'abord les m√©triques de baseline.

### Probl√®me

On utilise le m√™me mod√®le pour toutes les t√¢ches ‚Üí surco√ªt.

### Solution Simple (MVP)

Configuration manuelle avec 3 tiers :

```yaml
# server/config/models.yaml

tiers:
  fast:
    description: "Extraction, parsing, t√¢ches simples"
    anthropic: "claude-3-haiku-20240307"
    openai: "gpt-3.5-turbo"
    ollama: "llama3:8b"

  balanced:
    description: "Conversation, suggestions"
    anthropic: "claude-sonnet-4-20250514"
    openai: "gpt-4-turbo"
    ollama: "llama3:70b"

  powerful:
    description: "G√©n√©ration longue, raisonnement"
    anthropic: "claude-opus-4-20250514"
    openai: "gpt-4"

# Mapping task ‚Üí tier
task_routing:
  facts_extraction: "fast"
  step_render: "balanced"
  document_generation: "powerful"
  chat: "balanced"
```

### Utilisation

```python
# server/services/llm.py

class LLMService:
    def select_model(self, task_type: str) -> tuple[str, str]:
        """S√©lectionne provider et model selon la task"""
        tier = self.config["task_routing"].get(task_type, "balanced")
        tier_config = self.config["tiers"][tier]

        # Utiliser le provider pr√©f√©r√© de l'utilisateur
        preferred = self.user_config.get("preferred_provider", "anthropic")

        if preferred in tier_config:
            return (preferred, tier_config[preferred])

        # Fallback sur le premier disponible
        for provider, model in tier_config.items():
            if provider in self.available_providers:
                return (provider, model)

        raise NoModelAvailableError(f"No model available for tier {tier}")
```

### Crit√®res d'acceptation (Phase 5)

- [ ] Configuration YAML des tiers
- [ ] Routing par type de task
- [ ] Logs indiquent le mod√®le s√©lectionn√©
- [ ] M√©triques de co√ªt par tier
- [ ] User peut override via config

### Dur√©e estim√©e : 2 jours

---

## Planning R√©vis√©

```
Semaine 1:
‚îú‚îÄ‚îÄ [0] Instrumentation & Baseline (0.5j)
‚îú‚îÄ‚îÄ [A] Nettoyage dual workflow (1.5j)
‚îî‚îÄ‚îÄ [B] √âliminer double appels (2j)
    ‚îî‚îÄ‚îÄ Buffer/Tests (1j)

Semaine 2:
‚îú‚îÄ‚îÄ [C] Streaming & Feedback Visuel (3j)
‚îî‚îÄ‚îÄ [E] Compl√©ter i18n (2j)

Semaine 3:
‚îú‚îÄ‚îÄ [D] Knowledge Pipeline (4j)
‚îî‚îÄ‚îÄ Tests d'int√©gration (1j)

Phase 5 (plus tard):
‚îú‚îÄ‚îÄ [F] Context Enrichment Lite (3j)
‚îî‚îÄ‚îÄ [G] Model Router (2j)
```

---

## M√©triques de Succ√®s

| M√©trique | Baseline (√† mesurer) | Objectif Phase 2 | Objectif Phase 5 |
|----------|---------------------|------------------|------------------|
| Time to First Token | ~3-5s ? | < 1s | < 500ms |
| Tokens contexte moyen | ~10000 ? | < 3000 | < 2000 |
| Appels LLM par workflow start | 2 | 1 | 1 |
| Strings hardcod√©es | ~50+ | 0 | 0 |
| Co√ªt moyen par session | 100% | 70% | 50% |

---

## Tests √† √âcrire

### Unit Tests
- [ ] `services/metrics.test.py` - Collecte m√©triques
- [ ] `knowledge/extractor.test.py` - Extraction de facts
- [ ] `knowledge/context_builder.test.py` - Construction contexte
- [ ] `stores/chatStore.test.ts` - Nouveau store chat

### Integration Tests
- [ ] Workflow start ‚Üí premier step (1 seul appel)
- [ ] Streaming SSE end-to-end
- [ ] Cache invalidation sur modification document
- [ ] Transition Chat ‚Üí Workflow

### E2E Tests
- [ ] D√©marrer un workflow, v√©rifier latence < 3s
- [ ] Changer de langue, v√©rifier toutes les strings
- [ ] Cr√©er un document, v√©rifier extraction dans cache

---

## Risques et Mitigations

| Risque | Impact | Mitigation |
|--------|--------|------------|
| Cache invalidation incorrecte | Facts obsol√®tes | Hash + trigger on save |
| Streaming coupe en milieu de JSON | Frontend crash | Parser tol√©rant |
| Provider sans streaming | UX d√©grad√©e | Fallback polling |
| Thinking non support√© | Pas de feedback | Indicateur g√©n√©rique |

---

## Notes

- **Ne pas casser l'existant** : Feature flags si n√©cessaire
- **Instrumentation d'abord** : Impossible d'am√©liorer sans mesurer
- **Cleanup avant features** : A avant B/C √©vite la dette technique
- **Streaming = impact UX majeur** : Priorit√© sur les optimisations backend
