# TASK: LLM Speed Remediation (Workflow + Chat)

## Statut

- **PrioritÃ©** : Critical
- **Phase** : 2 - Studio Core
- **DÃ©pendances** : P2.0, P2.6, P2.7, P2.10
- **Date crÃ©ation** : 2026-01-26
- **DerniÃ¨re rÃ©vision** : 2026-01-26

---

## RÃ©sumÃ© ExÃ©cutif

Cette task adresse les **rÃ©gressions critiques** identifiÃ©es lors de la review architecturale du web-ui. L'objectif principal est d'atteindre une **vitesse perÃ§ue comparable Ã  Cursor** en rÃ©duisant drastiquement le temps avant le premier feedback visible (TTFT).

### Objectifs Quantifiables

| MÃ©trique | Baseline EstimÃ© | Objectif | AmÃ©lioration |
|----------|-----------------|----------|--------------|
| Time to First Token (TTFT) | ~3-5s | < 1s | -70% |
| Appels LLM au start | 2 | 1 | -50% |
| Taille contexte (tokens) | ~10000 | < 3000 | -70% |
| CoÃ»t par session | 100% | 60% | -40% |

---

## ProblÃ¨mes IdentifiÃ©s (Review)

### Vue d'Ensemble

| # | ProblÃ¨me | Impact | SÃ©vÃ©ritÃ© | Root Cause |
|---|----------|--------|----------|------------|
| 1 | Double appel LLM au dÃ©marrage | Latence x2, coÃ»t x2 | ğŸ”´ Critical | `/start` puis `/step` sÃ©parÃ©ment |
| 2 | Pas de streaming frontend | UX "freeze", TTFT perÃ§u trop long | ğŸ”´ Critical | Frontend attend rÃ©ponse complÃ¨te |
| 3 | Contexte trop volumineux | Tokens gaspillÃ©s, rÃ©ponses lentes | ğŸ”´ Critical | Documents entiers dans prompts |
| 4 | Deux systÃ¨mes workflow | Code dupliquÃ©, confusion | ğŸŸ  High | WebSocket chat + REST steps |
| 5 | i18n incomplÃ¨te | Strings FR/EN mÃ©langÃ©es | ğŸŸ  High | Pas de process d'audit |
| 6 | Knowledge context mal branchÃ© | Contexte vide ou cache cassÃ© | ğŸŸ¡ Medium | `project_path` au lieu de `project_id` |

### DÃ©tail des ProblÃ¨mes

#### ProblÃ¨me 1 : Double Appel LLM

**Flux Actuel (Mauvais)**
```
User clique "Start"
    â”‚
    â–¼
POST /api/workflows/start â”€â”€â”€â”€â”€â”€â–º Appel LLM #1 (crÃ©ation session)
    â”‚
    â”‚  returns session_id
    â–¼
GET /api/workflows/{id}/step â”€â”€â”€â–º Appel LLM #2 (render step)
    â”‚
    â”‚  returns step_data
    â–¼
Frontend affiche step
```

**ProblÃ¨me** : 2 appels LLM pour afficher le premier step = latence doublÃ©e.

**Preuve** : Dans `WorkflowStepContainer.tsx`:
```typescript
useEffect(() => {
  if (activeSession && activeSession.id !== lastSessionIdRef.current) {
    fetchStep()  // â† Appel LLM #2 systÃ©matique
  }
}, [activeSession, fetchStep])
```

#### ProblÃ¨me 2 : Pas de Streaming Frontend

**SymptÃ´me** : L'utilisateur voit un spinner pendant 3-5 secondes sans feedback.

**Cause** : Le backend supporte le streaming mais le frontend :
1. N'utilise pas SSE (Server-Sent Events)
2. Attend la rÃ©ponse JSON complÃ¨te
3. N'affiche rien pendant la gÃ©nÃ©ration

**Code Actuel** :
```typescript
// workflowStore.ts - attend la rÃ©ponse complÃ¨te
const response = await api.get(`/api/workflows/session/${sessionId}/step`)
set({ currentStepData: response.data })
```

#### ProblÃ¨me 3 : Contexte Trop Volumineux

**Mesure estimÃ©e** : ~10000 tokens de contexte moyen

**Contenu envoyÃ© actuellement** :
- Brief complet (2000-5000 tokens)
- GDD complet (10000-20000 tokens si existant)
- Historique conversation
- Instructions systÃ¨me

**Impact** :
- TTFT plus long (plus de tokens Ã  traiter)
- CoÃ»t Ã©levÃ© (facturÃ© au token)
- Risque de dÃ©passement de limite

#### ProblÃ¨me 4 : SystÃ¨mes Dual Workflow

**Architecture Actuelle (Confusion)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  workflowStore  â”‚         â”‚  workflowStore  â”‚
â”‚  (mode chat)    â”‚         â”‚  (mode step)    â”‚
â”‚                 â”‚         â”‚                 â”‚
â”‚ - WebSocket     â”‚         â”‚ - REST API      â”‚
â”‚ - Messages[]    â”‚         â”‚ - StepData      â”‚
â”‚ - streaming     â”‚         â”‚ - submit        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
            Code dupliquÃ©,
            maintenance difficile
```

#### ProblÃ¨me 5 : i18n IncomplÃ¨te

**Exemples de strings hardcodÃ©es** :
```typescript
// WorkflowStepContainer.tsx
<span>DÃ©marrage...</span>
<span>PrÃ©paration du workflow...</span>

// SuggestionCards.tsx
<p>Choisissez une option</p>

// AgentChatPanel.tsx
"L'agent rÃ©flÃ©chit..."
```

#### ProblÃ¨me 6 : Knowledge Context Mal BranchÃ©

**Bug identifiÃ© dans** `engine.py`:
```python
# INCORRECT :
context = await knowledge_service.build_context(project_id=project_path)

# CORRECT :
context = await knowledge_service.build_context(project_id=project_id)
```

**Impact** : Le cache facts ne fonctionne pas car la clÃ© est incorrecte.

---

## Sous-Tasks

### Vue d'Ensemble

| # | Sous-task | PrioritÃ© | ComplexitÃ© | DurÃ©e | DÃ©pendances |
|---|-----------|----------|------------|-------|-------------|
| 0 | Instrumentation Baseline | ğŸ”´ Critical | Basse | 0.5j | - |
| A | Nettoyage Dual Workflow | ğŸ”´ Critical | Moyenne | 1.5j | 0 |
| B | Single Call au Start | ğŸ”´ Critical | Moyenne | 1j | A |
| C | Streaming + Thinking UI | ğŸ”´ Critical | Haute | 3j | B |
| D | Contexte Compact | ğŸŸ  High | Moyenne | 2j | B |
| E | ComplÃ©ter i18n | ğŸŸ¡ Medium | Basse | 1j | C |

**Total estimÃ© : 9 jours**

### Graphe de DÃ©pendances

```
[0] Instrumentation Baseline
         â”‚
         â–¼
[A] Nettoyage Dual Workflow
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                      â–¼
[B] Single Call            [E] i18n
         â”‚                      â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
         â–¼          â–¼           â”‚
[C] Streaming   [D] Contexte    â”‚
         â”‚          â”‚           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
              âœ… DONE
```

---

## Sous-Task 0 : Instrumentation Baseline

### Objectif

Mesurer l'Ã©tat actuel **AVANT** toute modification pour :
1. Quantifier les amÃ©liorations
2. Identifier les goulots d'Ã©tranglement
3. Ã‰tablir des objectifs rÃ©alistes

### MÃ©triques Ã  Capturer

| MÃ©trique | Description | OÃ¹ mesurer | Comment |
|----------|-------------|------------|---------|
| TTFT | Time to First Token | Backend + Frontend | Timestamp start â†’ 1er chunk |
| TTC | Time to Complete | Frontend | Start â†’ rÃ©ponse complÃ¨te |
| Input Tokens | Tokens envoyÃ©s au LLM | Backend | Count avant appel |
| Output Tokens | Tokens reÃ§us du LLM | Backend | Count aprÃ¨s appel |
| Context Size | Taille du contexte | Backend | Len(context_string) |
| Calls/Session | Appels LLM par session | Backend | Counter |
| Cache Hit Rate | EfficacitÃ© du cache | Backend | hits / (hits + misses) |

### Fichiers Ã  CrÃ©er

| Fichier | Description |
|---------|-------------|
| `server/services/metrics.py` | Service de mÃ©triques singleton |
| `server/api/metrics.py` | Endpoints API pour rapports |
| `src/hooks/useMetrics.ts` | Hook frontend pour TTFT perÃ§u |

### SpÃ©cifications Techniques

#### Backend : `metrics.py`

```python
# server/services/metrics.py

import time
import logging
import statistics
from dataclasses import dataclass, field
from typing import Optional, Literal
from datetime import datetime
import json

logger = logging.getLogger("metrics")

TaskType = Literal[
    "workflow_start",
    "step_render", 
    "facts_extraction",
    "chat_response",
    "document_generation"
]

@dataclass
class LLMCallMetrics:
    """MÃ©triques d'un appel LLM individuel"""
    
    # Identifiants
    session_id: str
    call_id: str
    task_type: TaskType
    
    # Timing
    start_time: float = field(default_factory=time.time)
    time_to_first_token: Optional[float] = None
    time_to_complete: Optional[float] = None
    
    # Tokens
    input_tokens: int = 0
    output_tokens: int = 0
    context_size_chars: int = 0
    context_size_tokens: int = 0  # EstimÃ© : chars / 4
    
    # Provider info
    provider: str = ""
    model: str = ""
    
    # Status
    success: bool = True
    error_message: Optional[str] = None
    
    def mark_first_token(self):
        """Marque la rÃ©ception du premier token"""
        if self.time_to_first_token is None:
            self.time_to_first_token = time.time() - self.start_time
    
    def mark_complete(
        self, 
        input_tokens: int, 
        output_tokens: int,
        success: bool = True,
        error: str = None
    ):
        """Marque la fin de l'appel et log les mÃ©triques"""
        self.time_to_complete = time.time() - self.start_time
        self.input_tokens = input_tokens
        self.output_tokens = output_tokens
        self.success = success
        self.error_message = error
        self._log()
    
    def _log(self):
        """Log structurÃ© pour analyse"""
        log_data = {
            "event": "LLM_CALL",
            "session_id": self.session_id,
            "call_id": self.call_id,
            "task_type": self.task_type,
            "ttft": round(self.time_to_first_token, 3) if self.time_to_first_token else None,
            "ttc": round(self.time_to_complete, 3) if self.time_to_complete else None,
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
            "context_chars": self.context_size_chars,
            "context_tokens_est": self.context_size_tokens,
            "provider": self.provider,
            "model": self.model,
            "success": self.success,
            "error": self.error_message,
            "timestamp": datetime.now().isoformat()
        }
        
        if self.success:
            logger.info(json.dumps(log_data))
        else:
            logger.error(json.dumps(log_data))
    
    def to_dict(self) -> dict:
        """SÃ©rialise pour API"""
        return {
            "session_id": self.session_id,
            "call_id": self.call_id,
            "task_type": self.task_type,
            "ttft": self.time_to_first_token,
            "ttc": self.time_to_complete,
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
            "context_size_chars": self.context_size_chars,
            "provider": self.provider,
            "model": self.model,
            "success": self.success
        }


class MetricsCollector:
    """
    Collecteur de mÃ©triques singleton.
    
    Stocke les mÃ©triques en mÃ©moire pour la session serveur.
    Pour persistance, utiliser le logging structurÃ©.
    """
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._calls: list[LLMCallMetrics] = []
            cls._instance._cache_stats = {"hits": 0, "misses": 0}
            cls._instance._session_start = time.time()
        return cls._instance
    
    def new_call(
        self, 
        session_id: str,
        task_type: TaskType,
        provider: str = "",
        model: str = "",
        context_size: int = 0
    ) -> LLMCallMetrics:
        """CrÃ©e une nouvelle mÃ©trique d'appel"""
        call = LLMCallMetrics(
            session_id=session_id,
            call_id=f"{session_id}_{len(self._calls)}",
            task_type=task_type,
            provider=provider,
            model=model,
            context_size_chars=context_size,
            context_size_tokens=context_size // 4
        )
        self._calls.append(call)
        return call
    
    def record_cache_hit(self):
        """Enregistre un cache hit"""
        self._cache_stats["hits"] += 1
    
    def record_cache_miss(self):
        """Enregistre un cache miss"""
        self._cache_stats["misses"] += 1
    
    def get_baseline_report(self) -> dict:
        """
        GÃ©nÃ¨re un rapport baseline des mÃ©triques.
        
        Returns:
            dict avec statistiques agrÃ©gÃ©es
        """
        if not self._calls:
            return {
                "error": "No calls recorded",
                "uptime_seconds": time.time() - self._session_start
            }
        
        # Filtrer les appels rÃ©ussis
        successful = [c for c in self._calls if c.success]
        
        # TTFT stats
        ttfts = [c.time_to_first_token for c in successful if c.time_to_first_token]
        
        # TTC stats
        ttcs = [c.time_to_complete for c in successful if c.time_to_complete]
        
        # Token stats
        input_tokens = [c.input_tokens for c in successful]
        output_tokens = [c.output_tokens for c in successful]
        context_sizes = [c.context_size_chars for c in successful]
        
        # Calls per session
        sessions = set(c.session_id for c in self._calls)
        
        # Par task type
        by_task = {}
        for task_type in ["workflow_start", "step_render", "facts_extraction", "chat_response"]:
            task_calls = [c for c in successful if c.task_type == task_type]
            if task_calls:
                task_ttfts = [c.time_to_first_token for c in task_calls if c.time_to_first_token]
                by_task[task_type] = {
                    "count": len(task_calls),
                    "avg_ttft": statistics.mean(task_ttfts) if task_ttfts else None,
                    "avg_input_tokens": statistics.mean([c.input_tokens for c in task_calls])
                }
        
        # Cache stats
        total_cache = self._cache_stats["hits"] + self._cache_stats["misses"]
        cache_hit_rate = self._cache_stats["hits"] / total_cache if total_cache > 0 else 0
        
        return {
            # Totals
            "total_calls": len(self._calls),
            "successful_calls": len(successful),
            "failed_calls": len(self._calls) - len(successful),
            "unique_sessions": len(sessions),
            
            # TTFT
            "ttft": {
                "avg": round(statistics.mean(ttfts), 3) if ttfts else None,
                "p50": round(statistics.median(ttfts), 3) if ttfts else None,
                "p95": round(sorted(ttfts)[int(len(ttfts) * 0.95)] if len(ttfts) >= 20 else max(ttfts), 3) if ttfts else None,
                "min": round(min(ttfts), 3) if ttfts else None,
                "max": round(max(ttfts), 3) if ttfts else None
            },
            
            # TTC
            "ttc": {
                "avg": round(statistics.mean(ttcs), 3) if ttcs else None,
                "p50": round(statistics.median(ttcs), 3) if ttcs else None,
                "p95": round(sorted(ttcs)[int(len(ttcs) * 0.95)] if len(ttcs) >= 20 else max(ttcs), 3) if ttcs else None
            },
            
            # Tokens
            "tokens": {
                "avg_input": round(statistics.mean(input_tokens)) if input_tokens else 0,
                "avg_output": round(statistics.mean(output_tokens)) if output_tokens else 0,
                "avg_context_chars": round(statistics.mean(context_sizes)) if context_sizes else 0,
                "total_input": sum(input_tokens),
                "total_output": sum(output_tokens)
            },
            
            # Sessions
            "calls_per_session": round(len(self._calls) / len(sessions), 2) if sessions else 0,
            
            # By task type
            "by_task_type": by_task,
            
            # Cache
            "cache": {
                "hits": self._cache_stats["hits"],
                "misses": self._cache_stats["misses"],
                "hit_rate": round(cache_hit_rate, 3)
            },
            
            # Meta
            "uptime_seconds": round(time.time() - self._session_start),
            "generated_at": datetime.now().isoformat()
        }
    
    def get_recent_calls(self, limit: int = 50) -> list[dict]:
        """Retourne les N derniers appels"""
        return [c.to_dict() for c in self._calls[-limit:]]
    
    def reset(self):
        """Reset les mÃ©triques (pour tests)"""
        self._calls = []
        self._cache_stats = {"hits": 0, "misses": 0}


# Instance singleton
metrics = MetricsCollector()
```

#### Backend : API Endpoints

```python
# server/api/metrics.py

from fastapi import APIRouter, Query
from services.metrics import metrics

router = APIRouter(prefix="/api/metrics", tags=["metrics"])

@router.get("/baseline")
async def get_baseline_report():
    """
    GÃ©nÃ¨re un rapport baseline des mÃ©triques LLM.
    
    Utiliser avant/aprÃ¨s optimisations pour mesurer l'impact.
    """
    return metrics.get_baseline_report()

@router.get("/recent")
async def get_recent_calls(limit: int = Query(50, le=200)):
    """Retourne les N derniers appels LLM"""
    return {
        "calls": metrics.get_recent_calls(limit),
        "total_recorded": len(metrics._calls)
    }

@router.post("/reset")
async def reset_metrics():
    """Reset les mÃ©triques (dev/test uniquement)"""
    metrics.reset()
    return {"status": "reset", "message": "Metrics cleared"}
```

#### Frontend : Hook `useMetrics`

```typescript
// src/hooks/useMetrics.ts

import { useRef, useCallback } from 'react'

interface MetricEntry {
  id: string
  startTime: number
  firstTokenTime?: number
  completeTime?: number
  taskType: string
}

interface UseMetricsReturn {
  startMeasure: (id: string, taskType: string) => void
  markFirstToken: (id: string) => void
  markComplete: (id: string) => MetricEntry | null
  getMetrics: () => MetricEntry[]
}

export function useMetrics(): UseMetricsReturn {
  const metricsRef = useRef<Map<string, MetricEntry>>(new Map())
  
  const startMeasure = useCallback((id: string, taskType: string) => {
    metricsRef.current.set(id, {
      id,
      startTime: performance.now(),
      taskType
    })
    
    // Log pour debug
    console.debug(`[Metrics] Started: ${id} (${taskType})`)
  }, [])
  
  const markFirstToken = useCallback((id: string) => {
    const entry = metricsRef.current.get(id)
    if (entry && !entry.firstTokenTime) {
      entry.firstTokenTime = performance.now()
      const ttft = entry.firstTokenTime - entry.startTime
      console.debug(`[Metrics] First token: ${id} - TTFT: ${ttft.toFixed(0)}ms`)
    }
  }, [])
  
  const markComplete = useCallback((id: string): MetricEntry | null => {
    const entry = metricsRef.current.get(id)
    if (entry) {
      entry.completeTime = performance.now()
      const ttc = entry.completeTime - entry.startTime
      const ttft = entry.firstTokenTime 
        ? entry.firstTokenTime - entry.startTime 
        : null
      
      console.debug(
        `[Metrics] Complete: ${id} - ` +
        `TTFT: ${ttft?.toFixed(0) ?? 'N/A'}ms, ` +
        `TTC: ${ttc.toFixed(0)}ms`
      )
      
      // Envoyer au backend pour agrÃ©gation (optionnel)
      // sendToBackend(entry)
      
      return entry
    }
    return null
  }, [])
  
  const getMetrics = useCallback((): MetricEntry[] => {
    return Array.from(metricsRef.current.values())
  }, [])
  
  return { startMeasure, markFirstToken, markComplete, getMetrics }
}
```

### CritÃ¨res d'Acceptation

- [ ] MÃ©triques loggÃ©es pour chaque appel LLM (format JSON structurÃ©)
- [ ] Endpoint `/api/metrics/baseline` fonctionnel
- [ ] TTFT, TTC, tokens mesurÃ©s cÃ´tÃ© backend
- [ ] Hook frontend `useMetrics` disponible
- [ ] Overhead < 1ms par appel
- [ ] Logs consultables via `tail -f ~/.unreal_mcp/unreal_mcp.log | grep LLM_CALL`

### DurÃ©e EstimÃ©e : 0.5 jour

---

## Sous-Task A : Nettoyage Dual Workflow

### Objectif

SÃ©parer proprement les deux modes de communication :
- **workflowStore** : Step-based, REST uniquement
- **chatStore** : Conversation libre, SSE streaming

### ProblÃ¨me Actuel

```typescript
// workflowStore.ts - ACTUEL (mÃ©langÃ©)
interface WorkflowState {
  // Session step-based
  activeSession: Session | null
  currentStepData: StepData | null
  
  // Chat WebSocket (Ã  supprimer)
  ws: WebSocket | null
  messages: Message[]
  streamingContent: string
  
  // Actions mÃ©langÃ©es
  startWorkflow: () => Promise<void>
  connectWebSocket: () => void  // â† Ã€ SUPPRIMER
  sendMessage: () => void       // â† Ã€ SUPPRIMER
  submitStep: () => Promise<void>
}
```

### Architecture Cible

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         StudioPage                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                             â”‚
              â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    workflowStore     â”‚         â”‚      chatStore       â”‚
â”‚    (step-based)      â”‚         â”‚    (free-form)       â”‚
â”‚                      â”‚         â”‚                      â”‚
â”‚ â€¢ REST API only      â”‚         â”‚ â€¢ SSE streaming      â”‚
â”‚ â€¢ Session + Steps    â”‚         â”‚ â€¢ Messages history   â”‚
â”‚ â€¢ Submit responses   â”‚         â”‚ â€¢ Agent context      â”‚
â”‚ â€¢ No WebSocket       â”‚         â”‚ â€¢ Thinking indicator â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                â”‚
           â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WorkflowStepContainerâ”‚         â”‚     ChatPanel        â”‚
â”‚ WorkflowStepView     â”‚         â”‚     ChatMessage      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fichiers Ã  Modifier

| Fichier | Action | Description |
|---------|--------|-------------|
| `stores/workflowStore.ts` | Modifier | Supprimer WebSocket, garder REST |
| `stores/chatStore.ts` | CrÃ©er | Nouveau store pour chat libre |
| `api/chat.py` | CrÃ©er | Endpoint SSE pour chat |
| `components/chat/*` | Modifier | Utiliser chatStore |
| `components/workflow/*` | VÃ©rifier | Confirmer workflowStore |

### SpÃ©cifications Techniques

#### workflowStore.ts (NettoyÃ©)

```typescript
// src/stores/workflowStore.ts

import { create } from 'zustand'
import { devtools } from 'zustand/middleware'

// Types
interface WorkflowSession {
  id: string
  workflow_id: string
  project_id: string
  current_step: number
  total_steps: number
  status: 'active' | 'completed' | 'cancelled'
  created_at: string
  updated_at: string
}

interface StepRenderData {
  step_id: string
  step_number: number
  total_steps: number
  title: string
  agent: {
    id: string
    name: string
    avatar: string
    color?: string
  }
  intro_text: string
  questions: QuestionBlock[]
  suggestions: Suggestion[]
  prefilled: Record<string, string>
  can_skip: boolean
  skip_reason: string
  is_complete: boolean
}

interface QuestionBlock {
  id: string
  type: 'text' | 'textarea' | 'choice' | 'multi_choice' | 'choice_cards' | 'gauge' | 'emoji_scale'
  label: string
  required: boolean
  placeholder?: string
  options?: { id: string; label: string; icon?: string }[]
  suggestions?: string[]
  validation?: {
    min_length?: number
    max_length?: number
    pattern?: string
  }
}

interface Suggestion {
  id: string
  type: 'choice' | 'action'
  label: string
  value?: string
}

// Ã‰tat
interface WorkflowState {
  // Session
  activeSession: WorkflowSession | null
  currentStepData: StepRenderData | null
  
  // UI state
  isLoading: boolean
  isSubmitting: boolean
  error: string | null
  
  // Actions
  startWorkflow: (
    workflowId: string, 
    projectId: string, 
    projectPath: string,
    language?: string
  ) => Promise<void>
  
  fetchCurrentStep: () => Promise<void>
  
  submitStepResponse: (
    responses: Record<string, unknown>,
    skipped?: boolean
  ) => Promise<void>
  
  goBack: () => Promise<void>
  
  cancelWorkflow: () => void
  
  // Setters (pour SSE streaming)
  setCurrentStepData: (data: StepRenderData | null) => void
  
  // Reset
  reset: () => void
}

// Store
export const useWorkflowStore = create<WorkflowState>()(
  devtools(
    (set, get) => ({
      // Initial state
      activeSession: null,
      currentStepData: null,
      isLoading: false,
      isSubmitting: false,
      error: null,
      
      startWorkflow: async (workflowId, projectId, projectPath, language = 'fr') => {
        set({ isLoading: true, error: null })
        
        try {
          const response = await fetch('/api/workflows/start', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              workflow_id: workflowId,
              project_id: projectId,
              project_path: projectPath,
              language
            })
          })
          
          if (!response.ok) {
            throw new Error(`Failed to start workflow: ${response.statusText}`)
          }
          
          const data = await response.json()
          
          // IMPORTANT: Utiliser directement les donnÃ©es du step
          // PAS de second appel Ã  fetchCurrentStep()
          set({
            activeSession: data.session,
            currentStepData: data.step,  // â† Single call !
            isLoading: false
          })
          
        } catch (error) {
          set({ 
            error: error instanceof Error ? error.message : 'Unknown error',
            isLoading: false 
          })
        }
      },
      
      fetchCurrentStep: async () => {
        const { activeSession } = get()
        if (!activeSession) return
        
        set({ isLoading: true, error: null })
        
        try {
          const response = await fetch(
            `/api/workflows/session/${activeSession.id}/step`
          )
          
          if (!response.ok) {
            throw new Error('Failed to fetch step')
          }
          
          const stepData = await response.json()
          set({ currentStepData: stepData, isLoading: false })
          
        } catch (error) {
          set({ 
            error: error instanceof Error ? error.message : 'Unknown error',
            isLoading: false 
          })
        }
      },
      
      submitStepResponse: async (responses, skipped = false) => {
        const { activeSession } = get()
        if (!activeSession) return
        
        set({ isSubmitting: true, error: null })
        
        try {
          const response = await fetch(
            `/api/workflows/session/${activeSession.id}/step/submit`,
            {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ responses, skipped })
            }
          )
          
          if (!response.ok) {
            throw new Error('Failed to submit response')
          }
          
          const result = await response.json()
          
          if (result.complete) {
            // Workflow terminÃ©
            set({
              activeSession: { ...activeSession, status: 'completed' },
              currentStepData: null,
              isSubmitting: false
            })
          } else {
            // Prochain step
            set({
              activeSession: {
                ...activeSession,
                current_step: result.next_step.step_number
              },
              currentStepData: result.next_step,
              isSubmitting: false
            })
          }
          
        } catch (error) {
          set({
            error: error instanceof Error ? error.message : 'Unknown error',
            isSubmitting: false
          })
        }
      },
      
      goBack: async () => {
        const { activeSession } = get()
        if (!activeSession || activeSession.current_step <= 1) return
        
        set({ isLoading: true })
        
        try {
          const response = await fetch(
            `/api/workflows/session/${activeSession.id}/back`,
            { method: 'POST' }
          )
          
          if (!response.ok) {
            throw new Error('Failed to go back')
          }
          
          const result = await response.json()
          set({
            activeSession: {
              ...activeSession,
              current_step: result.step.step_number
            },
            currentStepData: result.step,
            isLoading: false
          })
          
        } catch (error) {
          set({
            error: error instanceof Error ? error.message : 'Unknown error',
            isLoading: false
          })
        }
      },
      
      cancelWorkflow: () => {
        set({
          activeSession: null,
          currentStepData: null,
          isLoading: false,
          isSubmitting: false,
          error: null
        })
      },
      
      setCurrentStepData: (data) => {
        set({ currentStepData: data })
      },
      
      reset: () => {
        set({
          activeSession: null,
          currentStepData: null,
          isLoading: false,
          isSubmitting: false,
          error: null
        })
      }
    }),
    { name: 'workflow-store' }
  )
)
```

#### chatStore.ts (Nouveau)

```typescript
// src/stores/chatStore.ts

import { create } from 'zustand'
import { devtools } from 'zustand/middleware'

interface ChatMessage {
  id: string
  role: 'user' | 'assistant' | 'system'
  content: string
  timestamp: Date
  isStreaming?: boolean
  thoughts?: string[]  // Pour thinking indicator
}

interface ChatState {
  // Messages
  messages: ChatMessage[]
  
  // Streaming
  isStreaming: boolean
  currentThoughts: string[]
  streamingContent: string
  
  // Agent context
  currentAgentId: string | null
  projectId: string | null
  
  // Actions
  sendMessage: (content: string) => Promise<void>
  setAgent: (agentId: string) => void
  setProject: (projectId: string) => void
  clearChat: () => void
  
  // Workflow trigger
  triggerWorkflow: (workflowId: string) => void
}

export const useChatStore = create<ChatState>()(
  devtools(
    (set, get) => ({
      messages: [],
      isStreaming: false,
      currentThoughts: [],
      streamingContent: '',
      currentAgentId: null,
      projectId: null,
      
      sendMessage: async (content: string) => {
        const { currentAgentId, projectId, messages } = get()
        
        // Ajouter message utilisateur
        const userMessage: ChatMessage = {
          id: crypto.randomUUID(),
          role: 'user',
          content,
          timestamp: new Date()
        }
        
        set({ 
          messages: [...messages, userMessage],
          isStreaming: true,
          currentThoughts: [],
          streamingContent: ''
        })
        
        try {
          // SSE stream
          const response = await fetch('/api/chat/stream', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              message: content,
              agent_id: currentAgentId,
              project_id: projectId,
              history: messages.slice(-10)  // Last 10 for context
            })
          })
          
          if (!response.ok) {
            throw new Error('Failed to send message')
          }
          
          const reader = response.body?.getReader()
          const decoder = new TextDecoder()
          
          if (!reader) {
            throw new Error('No response body')
          }
          
          let fullContent = ''
          const thoughts: string[] = []
          
          while (true) {
            const { done, value } = await reader.read()
            if (done) break
            
            const chunk = decoder.decode(value)
            const lines = chunk.split('\n').filter(line => line.startsWith('data: '))
            
            for (const line of lines) {
              const data = line.slice(6)  // Remove "data: "
              
              if (data === '[DONE]') {
                break
              }
              
              try {
                const parsed = JSON.parse(data)
                
                if (parsed.type === 'thinking') {
                  thoughts.push(parsed.content)
                  set({ currentThoughts: [...thoughts] })
                } else if (parsed.type === 'token') {
                  fullContent += parsed.content
                  set({ streamingContent: fullContent })
                }
              } catch {
                // Ignore parse errors
              }
            }
          }
          
          // Finaliser le message
          const assistantMessage: ChatMessage = {
            id: crypto.randomUUID(),
            role: 'assistant',
            content: fullContent,
            timestamp: new Date(),
            thoughts
          }
          
          set(state => ({
            messages: [...state.messages, assistantMessage],
            isStreaming: false,
            currentThoughts: [],
            streamingContent: ''
          }))
          
        } catch (error) {
          set({ isStreaming: false })
          console.error('Chat error:', error)
        }
      },
      
      setAgent: (agentId: string) => {
        set({ currentAgentId: agentId })
      },
      
      setProject: (projectId: string) => {
        set({ projectId })
      },
      
      clearChat: () => {
        set({
          messages: [],
          isStreaming: false,
          currentThoughts: [],
          streamingContent: ''
        })
      },
      
      triggerWorkflow: (workflowId: string) => {
        // Import dynamique pour Ã©viter circular dependency
        import('./workflowStore').then(({ useWorkflowStore }) => {
          const { startWorkflow } = useWorkflowStore.getState()
          const { projectId, currentAgentId } = get()
          
          if (projectId) {
            startWorkflow(workflowId, projectId, '', 'fr')
          }
        })
      }
    }),
    { name: 'chat-store' }
  )
)
```

#### Backend : Chat SSE Endpoint

```python
# server/api/chat.py

from fastapi import APIRouter, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional
import json

from services.llm import LLMService
from services.metrics import metrics

router = APIRouter(prefix="/api/chat", tags=["chat"])

class ChatRequest(BaseModel):
    message: str
    agent_id: Optional[str] = None
    project_id: Optional[str] = None
    history: list[dict] = []

@router.post("/stream")
async def stream_chat(request: ChatRequest):
    """
    SSE endpoint pour chat conversationnel.
    
    Events:
    - thinking: { type: "thinking", content: "..." }
    - token: { type: "token", content: "..." }
    - complete: { type: "complete" }
    - error: { type: "error", content: "..." }
    """
    
    llm = LLMService()
    
    async def event_generator():
        session_id = f"chat_{request.project_id or 'anonymous'}"
        call = metrics.new_call(
            session_id=session_id,
            task_type="chat_response",
            provider=llm.provider,
            model=llm.model
        )
        
        try:
            async for chunk in llm.stream_with_thinking(
                prompt=request.message,
                system=_build_chat_system(request.agent_id),
                history=request.history
            ):
                # Mark first token
                if chunk["type"] == "token":
                    call.mark_first_token()
                
                yield f"data: {json.dumps(chunk)}\n\n"
            
            yield "data: [DONE]\n\n"
            call.mark_complete(
                input_tokens=llm.last_input_tokens,
                output_tokens=llm.last_output_tokens
            )
            
        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'content': str(e)})}\n\n"
            call.mark_complete(0, 0, success=False, error=str(e))
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

def _build_chat_system(agent_id: str | None) -> str:
    """Construit le system prompt pour le chat"""
    base = "Tu es un assistant crÃ©atif pour le dÃ©veloppement de jeux vidÃ©o."
    
    if agent_id:
        # Charger persona de l'agent
        # TODO: Utiliser agent_service
        pass
    
    return base
```

### CritÃ¨res d'Acceptation

- [ ] `workflowStore` n'a plus de WebSocket ni `messages[]`
- [ ] `chatStore` crÃ©Ã© avec SSE streaming
- [ ] Transition workflow â†’ chat fonctionne
- [ ] Transition chat â†’ workflow fonctionne
- [ ] Aucune rÃ©gression sur workflows existants
- [ ] Tests unitaires passent

### DurÃ©e EstimÃ©e : 1.5 jours

---

## Sous-Task B : Single Call au Start

### Objectif

RÃ©duire de 2 Ã  1 le nombre d'appels LLM au dÃ©marrage d'un workflow.

### Flux Cible

```
User clique "Start"
    â”‚
    â–¼
POST /api/workflows/start â”€â”€â”€â”€â”€â”€â”€â–º 1 Appel LLM (session + step)
    â”‚
    â”‚  returns { session, step }
    â–¼
Frontend affiche step immÃ©diatement
```

### Fichiers Ã  Modifier

| Fichier | Action |
|---------|--------|
| `server/api/workflows.py` | `/start` retourne aussi `step` |
| `server/services/workflow/engine.py` | `start()` rend le premier step |
| `stores/workflowStore.ts` | Utilise `response.step` directement |
| `components/WorkflowStepContainer.tsx` | Ne fetch pas si data prÃ©sente |

### SpÃ©cifications Techniques

#### Backend : `/start` Enrichi

```python
# server/api/workflows.py

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional
from datetime import datetime

from services.workflow.engine import WorkflowEngine
from services.metrics import metrics

router = APIRouter(prefix="/api/workflows", tags=["workflows"])

class StartWorkflowRequest(BaseModel):
    workflow_id: str
    project_id: str
    project_path: str
    language: str = "fr"
    agent_id: Optional[str] = None

class SessionInfo(BaseModel):
    id: str
    workflow_id: str
    project_id: str
    current_step: int
    total_steps: int
    status: str
    created_at: datetime
    updated_at: datetime

class StartWorkflowResponse(BaseModel):
    session: SessionInfo
    step: dict  # StepRenderData

@router.post("/start", response_model=StartWorkflowResponse)
async def start_workflow(request: StartWorkflowRequest):
    """
    DÃ©marre un workflow et retourne la session + le premier step.
    
    IMPORTANT: Un seul appel LLM est fait ici.
    Le frontend ne doit PAS appeler /step ensuite.
    """
    engine = WorkflowEngine()
    
    # CrÃ©er mÃ©triques
    call = metrics.new_call(
        session_id=f"new_{request.project_id}",
        task_type="workflow_start",
        provider=engine.llm.provider,
        model=engine.llm.model
    )
    
    try:
        # Start retourne session + premier step rendu
        session, step_data = await engine.start(
            workflow_id=request.workflow_id,
            project_id=request.project_id,
            project_path=request.project_path,
            language=request.language,
            agent_id=request.agent_id
        )
        
        call.mark_complete(
            input_tokens=engine.llm.last_input_tokens,
            output_tokens=engine.llm.last_output_tokens
        )
        
        return {
            "session": SessionInfo(
                id=session.id,
                workflow_id=session.workflow_id,
                project_id=session.project_id,
                current_step=session.current_step,
                total_steps=session.total_steps,
                status=session.status,
                created_at=session.created_at,
                updated_at=session.updated_at
            ),
            "step": step_data
        }
        
    except Exception as e:
        call.mark_complete(0, 0, success=False, error=str(e))
        raise HTTPException(status_code=500, detail=str(e))
```

#### Engine : `start()` Rend le Step

```python
# server/services/workflow/engine.py

class WorkflowEngine:
    
    async def start(
        self,
        workflow_id: str,
        project_id: str,
        project_path: str,
        language: str = "fr",
        agent_id: str = None
    ) -> tuple[WorkflowSession, dict]:
        """
        DÃ©marre un workflow et rend le premier step.
        
        Returns:
            tuple: (session, step_render_data)
        """
        # 1. Charger le workflow YAML
        workflow = await self._load_workflow(workflow_id)
        
        # 2. CrÃ©er la session
        session = await self._create_session(
            workflow=workflow,
            project_id=project_id,
            project_path=project_path,
            language=language
        )
        
        # 3. Charger le contexte
        context = await self._load_context(
            project_id=project_id,  # â† CORRECT (pas project_path)
            workflow=workflow
        )
        
        # 4. Rendre le premier step
        first_step = workflow.steps[0]
        step_data = await self.step_renderer.render(
            session=session,
            step=first_step,
            context=context,
            language=language,
            agent_id=agent_id
        )
        
        return session, step_data
    
    async def _load_context(
        self, 
        project_id: str,
        workflow
    ) -> str:
        """Charge le contexte compact pour le LLM"""
        # IMPORTANT: Utiliser project_id, pas project_path !
        return await self.knowledge_service.build_context(
            project_id=project_id,
            focus=workflow.context_focus if hasattr(workflow, 'context_focus') else None
        )
```

#### Frontend : No Refetch

```typescript
// src/components/workflow/WorkflowStepContainer.tsx

import { useEffect, useRef } from 'react'
import { useWorkflowStore } from '@/stores/workflowStore'
import { WorkflowStepView } from './WorkflowStepView'
import { LoadingSpinner } from '@/components/common/LoadingSpinner'

export function WorkflowStepContainer() {
  const {
    activeSession,
    currentStepData,
    isLoading,
    error,
    fetchCurrentStep
  } = useWorkflowStore()
  
  // Ref pour Ã©viter les appels multiples
  const didFetchRef = useRef(false)
  
  useEffect(() => {
    // Ne fetch QUE si :
    // 1. On a une session active
    // 2. On n'a PAS de step data
    // 3. On n'a pas dÃ©jÃ  fetch
    if (activeSession && !currentStepData && !didFetchRef.current) {
      didFetchRef.current = true
      fetchCurrentStep()
    }
    
    // Reset le flag si la session change
    return () => {
      didFetchRef.current = false
    }
  }, [activeSession?.id])
  
  // Rendu
  if (error) {
    return <ErrorDisplay message={error} />
  }
  
  if (isLoading) {
    return <LoadingSpinner />
  }
  
  if (!currentStepData) {
    return null
  }
  
  // Render le step directement
  return <WorkflowStepView stepData={currentStepData} />
}
```

### VÃ©rification

Pour vÃ©rifier que seul 1 appel est fait :

```bash
# Terminal 1: Logs backend
tail -f ~/.unreal_mcp/unreal_mcp.log | grep "LLM_CALL"

# Terminal 2: Network Chrome
# Ouvrir DevTools > Network > filtrer "workflows"
# VÃ©rifier qu'il n'y a que POST /start, pas de GET /step
```

### CritÃ¨res d'Acceptation

- [ ] Un seul `LLM_CALL` loguÃ© au dÃ©marrage
- [ ] Pas de requÃªte GET `/step` aprÃ¨s `/start`
- [ ] Le step s'affiche en < 2s (hors latence LLM)
- [ ] Les workflows existants continuent de fonctionner
- [ ] Tests unitaires passent

### DurÃ©e EstimÃ©e : 1 jour

---

## Sous-Task C : Streaming + Thinking UI

### Objectif

Afficher un feedback visuel **immÃ©diat** pendant la gÃ©nÃ©ration LLM :
1. Indicateur "thinking" avec points d'analyse
2. Texte qui s'affiche progressivement

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LLM Provider                              â”‚
â”‚              (Anthropic, OpenAI, Ollama, etc.)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    stream token by token
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Backend (FastAPI)                           â”‚
â”‚                                                                  â”‚
â”‚  1. Inject thinking prompt                                       â”‚
â”‚  2. Parse <thinking>...</thinking>                              â”‚
â”‚  3. Emit SSE events                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                SSE: thinking | token | complete | error
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Frontend (React)                            â”‚
â”‚                                                                  â”‚
â”‚  useStreamingResponse()                                          â”‚
â”‚  â”œâ”€â”€ ThinkingIndicator (thoughts[])                             â”‚
â”‚  â””â”€â”€ StreamingText (text + cursor)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Design UI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ§  L'agent rÃ©flÃ©chit...                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ â€¢ Analyse du brief existant...                              â”‚â”‚
â”‚  â”‚ â€¢ Consultation de la section gameplay du GDD... â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ â€¢ Formulation des questions clÃ©s...              (animÃ©)    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RÃ©ponse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚
â”‚                                                                  â”‚
â”‚  BasÃ© sur ton brief, je te propose de dÃ©finir la vision         â”‚
â”‚  du jeu. Voici quelques questions pour commencer :              â”‚
â”‚  [texte qui s'affiche token par token]â–Š â†â”€â”€â”€â”€â”€â”€â”€ cursor         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fichiers Ã  CrÃ©er

| Fichier | Description |
|---------|-------------|
| `server/api/workflows.py` | Endpoint SSE `/stream/{session_id}` |
| `server/services/llm.py` | MÃ©thode `stream_with_thinking()` |
| `src/hooks/useStreamingResponse.ts` | Hook SSE |
| `src/components/common/ThinkingIndicator.tsx` | Composant thinking |
| `src/components/common/StreamingText.tsx` | Composant texte streaming |

### SpÃ©cifications Techniques

#### Format SSE

```
data: {"type":"thinking","content":"Analyse du brief existant..."}

data: {"type":"thinking","content":"Consultation de la section gameplay..."}

data: {"type":"thinking","content":"Formulation des questions clÃ©s..."}

data: {"type":"token","content":"BasÃ© "}

data: {"type":"token","content":"sur "}

data: {"type":"token","content":"ton brief"}

...

data: {"type":"complete","data":{"step_id":"vision","title":"Vision",...}}

data: [DONE]
```

#### Backend : SSE Endpoint

```python
# server/api/workflows.py

from fastapi import APIRouter, Query
from fastapi.responses import StreamingResponse
from enum import Enum
import json

class StreamEventType(str, Enum):
    THINKING = "thinking"
    TOKEN = "token"
    COMPLETE = "complete"
    ERROR = "error"

@router.get("/stream/{session_id}")
async def stream_step_response(
    session_id: str,
    project_path: str = Query(...)
):
    """
    SSE endpoint pour streaming des rÃ©ponses LLM.
    
    Events:
    - thinking: Points de rÃ©flexion de l'agent
    - token: Token de la rÃ©ponse
    - complete: RÃ©ponse complÃ¨te (step JSON)
    - error: Erreur
    
    Utiliser quand currentStepData est null aprÃ¨s start.
    """
    engine = WorkflowEngine()
    
    async def event_generator():
        try:
            # RÃ©cupÃ©rer la session
            session = await engine.get_session(session_id)
            if not session:
                yield f"data: {json.dumps({'type': 'error', 'content': 'Session not found'})}\n\n"
                return
            
            # Stream le step
            accumulated_json = ""
            async for chunk in engine.stream_step(session_id, project_path):
                if chunk["type"] == StreamEventType.THINKING:
                    yield f"data: {json.dumps(chunk)}\n\n"
                    
                elif chunk["type"] == StreamEventType.TOKEN:
                    accumulated_json += chunk["content"]
                    yield f"data: {json.dumps(chunk)}\n\n"
                    
                elif chunk["type"] == StreamEventType.COMPLETE:
                    # Parser le JSON accumulÃ©
                    try:
                        step_data = json.loads(accumulated_json)
                        yield f"data: {json.dumps({'type': 'complete', 'data': step_data})}\n\n"
                    except json.JSONDecodeError:
                        yield f"data: {json.dumps({'type': 'error', 'content': 'Invalid JSON response'})}\n\n"
            
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'content': str(e)})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Access-Control-Allow-Origin": "*"
        }
    )
```

#### Backend : LLM Stream avec Thinking

```python
# server/services/llm.py

from dataclasses import dataclass
from typing import AsyncGenerator, Literal, Optional

@dataclass
class StreamChunk:
    type: Literal["thinking", "token", "complete", "error"]
    content: Optional[str] = None
    data: Optional[dict] = None

class LLMService:
    
    # Prompt pour forcer le thinking
    THINKING_INSTRUCTION = """
Avant de rÃ©pondre, indique briÃ¨vement ce que tu analyses (2-3 points).
Format OBLIGATOIRE:
<thinking>
â€¢ Point 1
â€¢ Point 2
</thinking>

Puis donne ta rÃ©ponse en JSON strict.
"""
    
    async def stream_with_thinking(
        self,
        prompt: str,
        system: str = None,
        **kwargs
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        Stream la rÃ©ponse LLM avec extraction du bloc thinking.
        
        Le LLM doit rÃ©pondre avec:
        <thinking>
        â€¢ Point 1
        â€¢ Point 2
        </thinking>
        {JSON response}
        
        Yields:
            StreamChunk avec type thinking, token, ou complete
        """
        # Ajouter instruction thinking
        full_prompt = self.THINKING_INSTRUCTION + "\n\n" + prompt
        
        # Ã‰tat du parser
        in_thinking = False
        thinking_buffer = ""
        json_buffer = ""
        thinking_done = False
        
        async for token in self._raw_stream(full_prompt, system, **kwargs):
            # DÃ©tecter ouverture thinking
            if "<thinking>" in token and not thinking_done:
                in_thinking = True
                # Extraire ce qui est avant <thinking>
                before = token.split("<thinking>")[0]
                if before.strip():
                    json_buffer += before
                continue
            
            # DÃ©tecter fermeture thinking
            if "</thinking>" in token and in_thinking:
                in_thinking = False
                thinking_done = True
                
                # Extraire ce qui est aprÃ¨s </thinking>
                parts = token.split("</thinking>")
                thinking_buffer += parts[0]
                
                # Ã‰mettre les thoughts
                for line in thinking_buffer.strip().split('\n'):
                    line = line.strip().lstrip('â€¢-').strip()
                    if line:
                        yield StreamChunk(type="thinking", content=line)
                
                thinking_buffer = ""
                
                # Ce qui reste va dans le JSON
                if len(parts) > 1:
                    json_buffer += parts[1]
                continue
            
            # Accumuler
            if in_thinking:
                thinking_buffer += token
            else:
                json_buffer += token
                yield StreamChunk(type="token", content=token)
        
        # Marquer comme complet
        yield StreamChunk(type="complete", data=None)
    
    async def _raw_stream(
        self,
        prompt: str,
        system: str = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """Stream brut depuis le provider"""
        
        if self.provider == "anthropic":
            async for chunk in self._stream_anthropic(prompt, system, **kwargs):
                yield chunk
        elif self.provider == "openai":
            async for chunk in self._stream_openai(prompt, system, **kwargs):
                yield chunk
        elif self.provider == "ollama":
            async for chunk in self._stream_ollama(prompt, system, **kwargs):
                yield chunk
        else:
            # Fallback: pas de streaming
            result = await self.complete(prompt, system, **kwargs)
            yield result
    
    async def _stream_anthropic(self, prompt, system, **kwargs):
        """Stream depuis Anthropic"""
        import anthropic
        
        client = anthropic.AsyncAnthropic(api_key=self.api_key)
        
        async with client.messages.stream(
            model=self.model,
            max_tokens=kwargs.get("max_tokens", 4096),
            system=system or "",
            messages=[{"role": "user", "content": prompt}]
        ) as stream:
            async for text in stream.text_stream:
                yield text
    
    async def _stream_openai(self, prompt, system, **kwargs):
        """Stream depuis OpenAI"""
        import openai
        
        client = openai.AsyncOpenAI(api_key=self.api_key)
        
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})
        
        stream = await client.chat.completions.create(
            model=self.model,
            messages=messages,
            max_tokens=kwargs.get("max_tokens", 4096),
            stream=True
        )
        
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
    
    async def _stream_ollama(self, prompt, system, **kwargs):
        """Stream depuis Ollama"""
        import httpx
        
        async with httpx.AsyncClient() as client:
            async with client.stream(
                "POST",
                f"{self.ollama_base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "system": system or "",
                    "stream": True
                },
                timeout=60.0
            ) as response:
                async for line in response.aiter_lines():
                    if line:
                        data = json.loads(line)
                        if "response" in data:
                            yield data["response"]
```

#### Frontend : Hook useStreamingResponse

```typescript
// src/hooks/useStreamingResponse.ts

import { useState, useCallback, useRef, useEffect } from 'react'

interface StreamState {
  // Contenu
  text: string
  thoughts: string[]
  
  // Ã‰tat
  isStreaming: boolean
  isComplete: boolean
  error: string | null
  
  // DonnÃ©es finales (step JSON)
  finalData: Record<string, unknown> | null
}

interface UseStreamingResponseReturn extends StreamState {
  startStream: (url: string) => void
  reset: () => void
}

const initialState: StreamState = {
  text: '',
  thoughts: [],
  isStreaming: false,
  isComplete: false,
  error: null,
  finalData: null
}

export function useStreamingResponse(): UseStreamingResponseReturn {
  const [state, setState] = useState<StreamState>(initialState)
  const eventSourceRef = useRef<EventSource | null>(null)
  const mountedRef = useRef(true)
  
  // Cleanup on unmount
  useEffect(() => {
    mountedRef.current = true
    return () => {
      mountedRef.current = false
      eventSourceRef.current?.close()
    }
  }, [])
  
  const startStream = useCallback((url: string) => {
    // Cleanup previous
    if (eventSourceRef.current) {
      eventSourceRef.current.close()
    }
    
    // Reset state
    setState({
      ...initialState,
      isStreaming: true
    })
    
    // Create EventSource
    const eventSource = new EventSource(url)
    eventSourceRef.current = eventSource
    
    eventSource.onmessage = (event) => {
      if (!mountedRef.current) return
      
      // Handle [DONE]
      if (event.data === '[DONE]') {
        eventSource.close()
        setState(prev => ({
          ...prev,
          isStreaming: false,
          isComplete: true
        }))
        return
      }
      
      try {
        const data = JSON.parse(event.data)
        
        switch (data.type) {
          case 'thinking':
            setState(prev => ({
              ...prev,
              thoughts: [...prev.thoughts, data.content]
            }))
            break
            
          case 'token':
            setState(prev => ({
              ...prev,
              text: prev.text + data.content
            }))
            break
            
          case 'complete':
            setState(prev => ({
              ...prev,
              finalData: data.data,
              isComplete: true
            }))
            break
            
          case 'error':
            setState(prev => ({
              ...prev,
              error: data.content,
              isStreaming: false
            }))
            eventSource.close()
            break
        }
      } catch (e) {
        console.error('Failed to parse SSE event:', e)
      }
    }
    
    eventSource.onerror = (error) => {
      if (!mountedRef.current) return
      
      console.error('SSE error:', error)
      eventSource.close()
      setState(prev => ({
        ...prev,
        isStreaming: false,
        error: 'Connection lost'
      }))
    }
  }, [])
  
  const reset = useCallback(() => {
    eventSourceRef.current?.close()
    setState(initialState)
  }, [])
  
  return {
    ...state,
    startStream,
    reset
  }
}
```

#### Frontend : ThinkingIndicator

```typescript
// src/components/common/ThinkingIndicator.tsx

import { Brain, Loader2 } from 'lucide-react'
import { cn } from '@/lib/utils'
import { useTranslation } from '@/i18n/useI18n'
import { motion, AnimatePresence } from 'framer-motion'

interface ThinkingIndicatorProps {
  thoughts: string[]
  isActive: boolean
  className?: string
}

export function ThinkingIndicator({
  thoughts,
  isActive,
  className
}: ThinkingIndicatorProps) {
  const { t } = useTranslation()
  
  // Ne rien afficher si pas de thoughts et pas actif
  if (thoughts.length === 0 && !isActive) return null
  
  return (
    <motion.div
      initial={{ opacity: 0, y: -10 }}
      animate={{ opacity: 1, y: 0 }}
      exit={{ opacity: 0, y: -10 }}
      className={cn(
        "rounded-lg bg-muted/50 border border-muted-foreground/20 p-4 mb-4",
        "transition-all duration-300",
        isActive ? "opacity-100" : "opacity-60",
        className
      )}
    >
      {/* Header */}
      <div className="flex items-center gap-2 mb-3 text-sm font-medium">
        {isActive ? (
          <Loader2 className="h-4 w-4 animate-spin text-primary" />
        ) : (
          <Brain className="h-4 w-4 text-muted-foreground" />
        )}
        <span className={cn(
          isActive ? "text-primary" : "text-muted-foreground"
        )}>
          {isActive ? t('streaming.thinking') : t('streaming.thinkingDone')}
        </span>
      </div>
      
      {/* Thoughts list */}
      <ul className="space-y-2 text-sm">
        <AnimatePresence>
          {thoughts.map((thought, i) => (
            <motion.li
              key={i}
              initial={{ opacity: 0, x: -10 }}
              animate={{ opacity: 1, x: 0 }}
              transition={{ delay: i * 0.1 }}
              className="flex items-start gap-2"
            >
              <span className="text-muted-foreground mt-0.5 select-none">â€¢</span>
              <span className={cn(
                "transition-opacity duration-200",
                // Le dernier thought pulse si encore actif
                i === thoughts.length - 1 && isActive && "animate-pulse"
              )}>
                {thought}
              </span>
            </motion.li>
          ))}
        </AnimatePresence>
        
        {/* Placeholder si actif mais pas encore de thoughts */}
        {isActive && thoughts.length === 0 && (
          <motion.li
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            className="flex items-center gap-2 text-muted-foreground"
          >
            <span className="animate-pulse">â€¢</span>
            <span className="animate-pulse">{t('streaming.analyzing')}</span>
          </motion.li>
        )}
      </ul>
    </motion.div>
  )
}
```

#### Frontend : StreamingText

```typescript
// src/components/common/StreamingText.tsx

import ReactMarkdown from 'react-markdown'
import { cn } from '@/lib/utils'
import { motion } from 'framer-motion'

interface StreamingTextProps {
  text: string
  isStreaming: boolean
  className?: string
}

export function StreamingText({ 
  text, 
  isStreaming, 
  className 
}: StreamingTextProps) {
  
  if (!text && !isStreaming) return null
  
  return (
    <div className={cn(
      "prose prose-sm dark:prose-invert max-w-none",
      className
    )}>
      <ReactMarkdown
        components={{
          // Personnaliser le rendu si besoin
          p: ({ children }) => (
            <p className="mb-2 last:mb-0">{children}</p>
          )
        }}
      >
        {text}
      </ReactMarkdown>
      
      {/* Cursor clignotant pendant le streaming */}
      {isStreaming && (
        <motion.span
          initial={{ opacity: 0 }}
          animate={{ opacity: [0, 1, 0] }}
          transition={{ 
            duration: 0.8, 
            repeat: Infinity,
            ease: "easeInOut"
          }}
          className="inline-block w-2 h-5 bg-primary ml-0.5 align-text-bottom"
        />
      )}
    </div>
  )
}
```

#### Frontend : Wiring dans WorkflowStepContainer

```typescript
// src/components/workflow/WorkflowStepContainer.tsx (mise Ã  jour)

import { useEffect } from 'react'
import { useWorkflowStore } from '@/stores/workflowStore'
import { useStreamingResponse } from '@/hooks/useStreamingResponse'
import { WorkflowStepView } from './WorkflowStepView'
import { ThinkingIndicator } from '@/components/common/ThinkingIndicator'
import { StreamingText } from '@/components/common/StreamingText'
import { LoadingSpinner } from '@/components/common/LoadingSpinner'

export function WorkflowStepContainer() {
  const {
    activeSession,
    currentStepData,
    isLoading,
    error,
    setCurrentStepData
  } = useWorkflowStore()
  
  const {
    thoughts,
    text,
    isStreaming,
    isComplete,
    finalData,
    error: streamError,
    startStream,
    reset
  } = useStreamingResponse()
  
  // DÃ©marrer le stream si pas de step data
  useEffect(() => {
    if (activeSession && !currentStepData && !isStreaming && !isComplete) {
      const url = `/api/workflows/stream/${activeSession.id}?project_path=${encodeURIComponent(activeSession.project_path || '')}`
      startStream(url)
    }
  }, [activeSession?.id, currentStepData, isStreaming, isComplete])
  
  // Quand le stream est complet, utiliser les donnÃ©es
  useEffect(() => {
    if (isComplete && finalData) {
      setCurrentStepData(finalData as any)
      reset()
    }
  }, [isComplete, finalData])
  
  // Error handling
  if (error || streamError) {
    return <ErrorDisplay message={error || streamError || 'Unknown error'} />
  }
  
  // Loading initial
  if (isLoading && !isStreaming) {
    return <LoadingSpinner />
  }
  
  // Streaming mode
  if (isStreaming || (thoughts.length > 0 && !currentStepData)) {
    return (
      <div className="p-6 space-y-4">
        <ThinkingIndicator 
          thoughts={thoughts} 
          isActive={isStreaming} 
        />
        
        {text && (
          <StreamingText 
            text={text} 
            isStreaming={isStreaming} 
          />
        )}
      </div>
    )
  }
  
  // Render step complet
  if (currentStepData) {
    return <WorkflowStepView stepData={currentStepData} />
  }
  
  return null
}
```

### Fallback Sans Streaming

Pour les providers qui ne supportent pas le streaming, on utilise des phrases de thinking locales :

```typescript
// src/config/localThoughts.ts

export const LOCAL_THINKING_PHRASES = {
  fr: [
    "Analyse du contexte projet...",
    "Consultation des documents existants...",
    "PrÃ©paration des questions pertinentes...",
    "Structuration de la rÃ©ponse..."
  ],
  en: [
    "Analyzing project context...",
    "Reviewing existing documents...",
    "Preparing relevant questions...",
    "Structuring the response..."
  ],
  es: [
    "Analizando el contexto del proyecto...",
    "Revisando documentos existentes...",
    "Preparando preguntas relevantes...",
    "Estructurando la respuesta..."
  ]
}

export function getLocalThoughts(language: string = 'fr'): string[] {
  return LOCAL_THINKING_PHRASES[language] || LOCAL_THINKING_PHRASES.fr
}
```

```typescript
// Usage dans WorkflowStepContainer
useEffect(() => {
  if (activeSession && !currentStepData && !supportsStreaming) {
    // Afficher thoughts locales pendant le chargement
    const thoughts = getLocalThoughts(activeSession.language)
    let index = 0
    
    const interval = setInterval(() => {
      if (index < thoughts.length) {
        setLocalThoughts(prev => [...prev, thoughts[index]])
        index++
      } else {
        clearInterval(interval)
      }
    }, 800)
    
    return () => clearInterval(interval)
  }
}, [activeSession, currentStepData])
```

### CritÃ¨res d'Acceptation

- [ ] Premier token visible en < 1s aprÃ¨s l'appel LLM
- [ ] Thinking indicator avec 2-3 points
- [ ] Texte s'affiche progressivement
- [ ] Animation fluide (pas de saccades)
- [ ] Fonctionne avec Anthropic, OpenAI, Ollama
- [ ] Fallback gracieux si streaming non supportÃ©
- [ ] Pas de memory leak (EventSource fermÃ©)
- [ ] Tests: unit + integration

### DurÃ©e EstimÃ©e : 3 jours

---

## Sous-Task D : Contexte Compact

### Objectif

RÃ©duire la taille du contexte envoyÃ© au LLM de ~10000 tokens Ã  < 3000 tokens.

### StratÃ©gie

1. **Fix project_id** : Corriger le bug de branchement
2. **Facts only** : Utiliser les faits extraits, pas les documents bruts
3. **Max tokens** : Limiter par workflow
4. **Cache** : Ã‰viter les recalculs

### Fichiers Ã  Modifier

| Fichier | Action |
|---------|--------|
| `services/workflow/engine.py` | Corriger `project_id` |
| `services/knowledge/service.py` | VÃ©rifier interface |
| `services/knowledge/context_builder.py` | Ajouter `max_tokens` |
| `config/workflows/*.yaml` | Ajouter `max_context_tokens` |

### SpÃ©cifications Techniques

#### Fix project_id

```python
# server/services/workflow/engine.py

async def _load_context(
    self, 
    project_id: str,  # â† ParamÃ¨tre correct
    workflow: Workflow
) -> str:
    """Charge le contexte compact pour le LLM"""
    
    # CORRECT: utiliser project_id
    return await self.knowledge_service.build_context(
        project_id=project_id,  # â† PAS project_path !
        focus=workflow.context_focus if hasattr(workflow, 'context_focus') else None,
        max_tokens=workflow.max_context_tokens if hasattr(workflow, 'max_context_tokens') else 3000
    )
```

#### Context Builder avec limite

```python
# server/services/knowledge/context_builder.py

class ContextBuilder:
    """Construit un contexte minimal pour le LLM"""
    
    DEFAULT_MAX_TOKENS = 3000
    CHARS_PER_TOKEN = 4  # Approximation
    
    async def build_context(
        self,
        project_id: str,
        focus: str = None,
        max_tokens: int = None
    ) -> str:
        """
        Retourne un contexte compact pour le prompt.
        
        Args:
            project_id: ID du projet
            focus: Focus optionnel ("gameplay", "narrative", "technical")
            max_tokens: Limite de tokens (dÃ©faut: 3000)
        
        Returns:
            Contexte formatÃ© en markdown
        """
        max_tokens = max_tokens or self.DEFAULT_MAX_TOKENS
        max_chars = max_tokens * self.CHARS_PER_TOKEN
        
        # RÃ©cupÃ©rer les facts cachÃ©s
        brief_facts = await self.cache.get(project_id, "brief")
        gdd_facts = await self.cache.get(project_id, "gdd")
        
        if not brief_facts:
            # Pas de facts = pas de cache = recalculer
            self.metrics.record_cache_miss()
            # Fallback minimal
            return self._minimal_context(project_id)
        
        self.metrics.record_cache_hit()
        
        context_parts = []
        current_size = 0
        
        # 1. Toujours inclure le summary du brief (prioritÃ© haute)
        brief_section = self._format_brief_context(brief_facts)
        if current_size + len(brief_section) < max_chars:
            context_parts.append(brief_section)
            current_size += len(brief_section)
        
        # 2. GDD : filtrer par focus si spÃ©cifiÃ©
        if gdd_facts:
            gdd_section = self._format_gdd_context(gdd_facts, focus)
            if current_size + len(gdd_section) < max_chars:
                context_parts.append(gdd_section)
                current_size += len(gdd_section)
            elif focus:
                # Si trop gros, au moins le summary
                summary = f"## GDD Summary\n{gdd_facts.summary}"
                if current_size + len(summary) < max_chars:
                    context_parts.append(summary)
        
        # Assembler
        context = "\n\n".join(context_parts)
        
        # VÃ©rification finale et troncature si nÃ©cessaire
        if len(context) > max_chars:
            context = self._truncate_intelligently(context, max_chars)
        
        return context
    
    def _format_brief_context(self, facts) -> str:
        """Formate le brief de maniÃ¨re compacte"""
        f = facts.facts
        return f"""## Game Brief
**{f.get('game_name', 'Untitled')}** - {f.get('genre', 'Unknown')}
Target: {f.get('target_audience', 'N/A')}
Core Loop: {f.get('core_loop', 'N/A')}
USPs: {', '.join(f.get('unique_selling_points', []))}
Summary: {f.get('summary', '')}"""
    
    def _format_gdd_context(self, facts, focus: str = None) -> str:
        """Formate le GDD selon le focus"""
        if focus:
            relevant = self._filter_by_focus(facts.facts, focus)
            return f"## GDD ({focus})\n{json.dumps(relevant, indent=2)}"
        else:
            return f"## GDD Summary\n{facts.summary}"
    
    def _filter_by_focus(self, facts: dict, focus: str) -> dict:
        """Filtre les sections par focus"""
        focus_mapping = {
            "gameplay": ["mechanics", "systems", "progression", "combat", "controls"],
            "narrative": ["story", "characters", "world", "lore", "dialogue"],
            "technical": ["architecture", "performance", "platforms", "tools", "tech"]
        }
        
        relevant_keys = focus_mapping.get(focus, [])
        return {
            k: v for k, v in facts.items()
            if any(rk in k.lower() for rk in relevant_keys)
        }
    
    def _truncate_intelligently(self, context: str, max_chars: int) -> str:
        """Tronque intelligemment le contexte"""
        if len(context) <= max_chars:
            return context
        
        # Couper Ã  la derniÃ¨re phrase complÃ¨te
        truncated = context[:max_chars]
        last_period = truncated.rfind('.')
        last_newline = truncated.rfind('\n')
        
        cut_point = max(last_period, last_newline)
        if cut_point > max_chars * 0.8:  # Au moins 80% conservÃ©
            return truncated[:cut_point + 1] + "\n[...]"
        
        return truncated + "\n[...]"
    
    def _minimal_context(self, project_id: str) -> str:
        """Contexte minimal si pas de facts"""
        return f"""## Project Context
Project ID: {project_id}
No cached documents found. Please provide context in your questions."""
```

#### Configuration par Workflow

```yaml
# server/config/workflows/game-brief.yaml

id: game-brief
name: Game Brief Workshop
version: 2

# Context settings
max_context_tokens: 2500
context_focus: null  # Tout le brief

steps:
  - id: vision
    name: Vision du jeu
    context_focus: null  # HÃ©rite du workflow
    
  - id: gameplay
    name: Gameplay Core
    context_focus: gameplay  # Focus spÃ©cifique
    max_context_tokens: 3000  # Override
```

### MÃ©triques de RÃ©duction

| Document | Avant | AprÃ¨s | RÃ©duction |
|----------|-------|-------|-----------|
| Brief complet | ~3000 tokens | ~500 tokens | -83% |
| GDD complet | ~15000 tokens | ~1000 tokens | -93% |
| Historique | ~2000 tokens | ~500 tokens | -75% |
| **Total** | **~20000** | **~2000** | **-90%** |

### CritÃ¨res d'Acceptation

- [ ] `project_id` correctement passÃ© au knowledge service
- [ ] Contexte moyen < 3000 tokens
- [ ] Cache hit rate > 80% aprÃ¨s warm-up
- [ ] Pas de rÃ©gression sur la qualitÃ© des rÃ©ponses
- [ ] Config max_tokens par workflow fonctionnelle

### DurÃ©e EstimÃ©e : 2 jours

---

## Sous-Task E : ComplÃ©ter i18n

### Objectif

Remplacer toutes les strings hardcodÃ©es par des clÃ©s de traduction.

### Audit des Composants

| Fichier | Strings trouvÃ©es | Action |
|---------|------------------|--------|
| `WorkflowStepContainer.tsx` | "DÃ©marrage...", "PrÃ©paration..." | Remplacer |
| `ThinkingIndicator.tsx` | "L'agent rÃ©flÃ©chit..." | DÃ©jÃ  i18n âœ“ |
| `StreamingText.tsx` | Aucune | OK âœ“ |
| `SuggestionCards.tsx` | "Suggestions" | Remplacer |
| `WorkflowStepView.tsx` | "Continuer", "Passer" | VÃ©rifier |

### ClÃ©s Ã  Ajouter

```typescript
// src/i18n/translations.ts

export const translations = {
  en: {
    // Streaming
    'streaming.thinking': 'The agent is thinking...',
    'streaming.thinkingDone': 'Analysis complete',
    'streaming.analyzing': 'Analyzing...',
    'streaming.error': 'An error occurred',
    'streaming.connectionLost': 'Connection lost',
    'streaming.reconnecting': 'Reconnecting...',
    
    // Workflow
    'workflow.starting': 'Starting...',
    'workflow.preparing': 'Preparing workflow...',
    'workflow.loading': 'Loading step...',
    'workflow.submitting': 'Submitting...',
    'workflow.goBack': 'Go back',
    'workflow.continue': 'Continue',
    'workflow.skip': 'Skip this step',
    'workflow.complete': 'Complete',
    'workflow.error': 'An error occurred',
    'workflow.cancel': 'Cancel workflow',
    
    // Suggestions
    'suggestions.title': 'Suggestions',
    'suggestions.empty': 'No suggestions available',
    
    // Errors
    'error.generic': 'Something went wrong',
    'error.network': 'Network error',
    'error.timeout': 'Request timed out',
    'error.retry': 'Try again',
  },
  
  fr: {
    // Streaming
    'streaming.thinking': "L'agent rÃ©flÃ©chit...",
    'streaming.thinkingDone': 'Analyse terminÃ©e',
    'streaming.analyzing': 'Analyse en cours...',
    'streaming.error': 'Une erreur est survenue',
    'streaming.connectionLost': 'Connexion perdue',
    'streaming.reconnecting': 'Reconnexion...',
    
    // Workflow
    'workflow.starting': 'DÃ©marrage...',
    'workflow.preparing': 'PrÃ©paration du workflow...',
    'workflow.loading': 'Chargement...',
    'workflow.submitting': 'Envoi en cours...',
    'workflow.goBack': 'Retour',
    'workflow.continue': 'Continuer',
    'workflow.skip': 'Passer cette Ã©tape',
    'workflow.complete': 'Terminer',
    'workflow.error': 'Une erreur est survenue',
    'workflow.cancel': 'Annuler',
    
    // Suggestions
    'suggestions.title': 'Suggestions',
    'suggestions.empty': 'Aucune suggestion disponible',
    
    // Errors
    'error.generic': 'Une erreur est survenue',
    'error.network': 'Erreur rÃ©seau',
    'error.timeout': 'DÃ©lai dÃ©passÃ©',
    'error.retry': 'RÃ©essayer',
  },
  
  es: {
    // Streaming
    'streaming.thinking': 'El agente estÃ¡ pensando...',
    'streaming.thinkingDone': 'AnÃ¡lisis completo',
    'streaming.analyzing': 'Analizando...',
    'streaming.error': 'Se produjo un error',
    'streaming.connectionLost': 'ConexiÃ³n perdida',
    'streaming.reconnecting': 'Reconectando...',
    
    // Workflow
    'workflow.starting': 'Iniciando...',
    'workflow.preparing': 'Preparando workflow...',
    'workflow.loading': 'Cargando...',
    'workflow.submitting': 'Enviando...',
    'workflow.goBack': 'Volver',
    'workflow.continue': 'Continuar',
    'workflow.skip': 'Saltar este paso',
    'workflow.complete': 'Completar',
    'workflow.error': 'Se produjo un error',
    'workflow.cancel': 'Cancelar',
    
    // Suggestions
    'suggestions.title': 'Sugerencias',
    'suggestions.empty': 'No hay sugerencias disponibles',
    
    // Errors
    'error.generic': 'Algo saliÃ³ mal',
    'error.network': 'Error de red',
    'error.timeout': 'Tiempo de espera agotado',
    'error.retry': 'Intentar de nuevo',
  }
}
```

### CritÃ¨res d'Acceptation

- [ ] Aucune string UI hardcodÃ©e dans les composants modifiÃ©s
- [ ] Toutes les nouvelles clÃ©s existent en EN, FR, ES
- [ ] Langue par dÃ©faut respectÃ©e au chargement
- [ ] Pas de rÃ©gression sur les traductions existantes
- [ ] Test visuel dans les 3 langues

### DurÃ©e EstimÃ©e : 1 jour

---

## Plan de Migration

### Ordre d'ImplÃ©mentation

```
Semaine 1 (3.5j):
â”œâ”€â”€ [0] Instrumentation (0.5j)
â”‚   â””â”€â”€ MÃ©triques baseline capturÃ©es
â”œâ”€â”€ [A] Nettoyage dual (1.5j)
â”‚   â””â”€â”€ Stores sÃ©parÃ©s, WS supprimÃ©
â””â”€â”€ [B] Single call (1j)
    â””â”€â”€ 1 appel au start
    â””â”€â”€ Buffer tests (0.5j)

Semaine 2 (4.5j):
â”œâ”€â”€ [C] Streaming + Thinking (3j)
â”‚   â””â”€â”€ SSE fonctionnel
â”‚   â””â”€â”€ UI thinking + cursor
â”œâ”€â”€ [D] Contexte compact (1j)
â”‚   â””â”€â”€ Facts only, max_tokens
â””â”€â”€ [E] i18n (0.5j)
    â””â”€â”€ Strings remplacÃ©es
```

### Feature Flags

```yaml
# config/features.yaml

features:
  # Streaming SSE pour workflows
  enable_step_streaming:
    default: false
    description: "Enable SSE streaming for workflow steps"
    rollout:
      - dev: true
      - staging: true
      - production: false  # Activer aprÃ¨s validation
  
  # Contexte compact
  enable_compact_context:
    default: true
    description: "Use facts-based compact context instead of full documents"
  
  # Fallback thinking local
  enable_local_thinking:
    default: true
    description: "Show local thinking phrases when streaming unavailable"
  
  # MÃ©triques dÃ©taillÃ©es
  enable_detailed_metrics:
    default: true
    description: "Log detailed LLM metrics"
```

```python
# Usage dans le code
from config.features import is_enabled

if is_enabled("enable_step_streaming"):
    # Utiliser SSE
    async for chunk in engine.stream_step(session_id):
        yield chunk
else:
    # Fallback: rÃ©ponse complÃ¨te
    result = await engine.render_step(session_id)
    yield {"type": "complete", "data": result}
```

### Rollback Strategy

| Sous-task | Rollback |
|-----------|----------|
| Instrumentation | DÃ©sactiver logging (impact: 0) |
| Dual cleanup | RÃ©activer WS dans store (nÃ©cessite code backup) |
| Single call | RÃ©activer fetchStep() dans useEffect |
| Streaming | Feature flag off â†’ fallback non-stream |
| Contexte | Feature flag off â†’ full documents |
| i18n | Aucun rollback nÃ©cessaire |

---

## MÃ©triques de SuccÃ¨s

### Objectifs Phase 2

| MÃ©trique | Baseline | Objectif | Validation |
|----------|----------|----------|------------|
| TTFT | ~3-5s | < 1s | `metrics.get_baseline_report()["ttft"]["avg"]` |
| Calls au start | 2 | 1 | Logs LLM_CALL count |
| Contexte tokens | ~10000 | < 3000 | Logs context_tokens_est |
| CoÃ»t/session | 100% | 60% | (input + output) * prix |
| Strings hardcodÃ©es | 50+ | 0 | Audit grep |

### Dashboard MÃ©triques

Endpoint `/api/metrics/baseline` retourne :

```json
{
  "ttft": {
    "avg": 0.89,
    "p50": 0.82,
    "p95": 1.4,
    "target": 1.0,
    "status": "âœ… PASS"
  },
  "calls_per_session": {
    "current": 1.2,
    "target": 1.0,
    "status": "âš ï¸ CLOSE"
  },
  "tokens": {
    "avg_context": 2450,
    "target": 3000,
    "status": "âœ… PASS"
  },
  "cost_reduction": {
    "current_vs_baseline": 0.58,
    "target": 0.60,
    "status": "âœ… PASS"
  }
}
```

---

## Tests

### Unit Tests

```python
# tests/test_metrics.py
def test_metrics_collector_singleton():
    m1 = MetricsCollector()
    m2 = MetricsCollector()
    assert m1 is m2

def test_ttft_measurement():
    call = metrics.new_call("test", "step_render")
    time.sleep(0.1)
    call.mark_first_token()
    assert 0.09 < call.time_to_first_token < 0.15

def test_baseline_report_empty():
    metrics.reset()
    report = metrics.get_baseline_report()
    assert "error" in report
```

```typescript
// tests/useStreamingResponse.test.ts
describe('useStreamingResponse', () => {
  it('should accumulate thinking events', async () => {
    const { result } = renderHook(() => useStreamingResponse())
    
    // Simulate SSE events
    act(() => result.current.startStream('/mock'))
    
    // Mock event
    mockEventSource.emit({ type: 'thinking', content: 'Test thought' })
    
    expect(result.current.thoughts).toContain('Test thought')
  })
})
```

### Integration Tests

```python
# tests/test_workflow_start.py
async def test_single_call_start():
    """VÃ©rifie qu'un seul appel LLM est fait au dÃ©marrage"""
    metrics.reset()
    
    response = await client.post("/api/workflows/start", json={
        "workflow_id": "game-brief",
        "project_id": "test-project",
        "project_path": "/tmp/test"
    })
    
    assert response.status_code == 200
    assert "session" in response.json()
    assert "step" in response.json()
    
    # VÃ©rifier qu'un seul appel a Ã©tÃ© fait
    report = metrics.get_baseline_report()
    assert report["total_calls"] == 1
```

### E2E Tests

```typescript
// cypress/e2e/workflow-streaming.cy.ts
describe('Workflow Streaming', () => {
  it('should show thinking indicator within 1s', () => {
    cy.visit('/studio')
    cy.get('[data-testid="start-workflow"]').click()
    
    // Thinking devrait apparaÃ®tre en < 1s
    cy.get('[data-testid="thinking-indicator"]', { timeout: 1000 })
      .should('be.visible')
  })
  
  it('should display streaming text', () => {
    cy.visit('/studio')
    cy.get('[data-testid="start-workflow"]').click()
    
    // Attendre le streaming text
    cy.get('[data-testid="streaming-text"]', { timeout: 5000 })
      .should('contain.text', '')  // Non vide
  })
})
```

---

## Risques et Mitigations

| Risque | Impact | ProbabilitÃ© | Mitigation |
|--------|--------|-------------|------------|
| Streaming coupe le JSON | Frontend crash | Moyenne | Parser tolÃ©rant + event complete |
| Provider sans streaming | UX dÃ©gradÃ©e | Basse | Fallback thinking local |
| Cache facts obsolÃ¨te | RÃ©ponses incohÃ©rentes | Moyenne | Hash invalidation + TTL |
| Contexte trop court | RÃ©ponses pauvres | Basse | Fallback summary + docs critiques |
| RÃ©gression workflows | FonctionnalitÃ© cassÃ©e | Haute | Tests E2E + feature flags |
| Memory leak SSE | Performance | Moyenne | Cleanup EventSource dans useEffect |

---

## Checklist Finale

### Backend

- [ ] `services/metrics.py` crÃ©Ã© avec singleton
- [ ] `api/metrics.py` endpoints fonctionnels
- [ ] `api/workflows.py` SSE endpoint ajoutÃ©
- [ ] `services/llm.py` stream_with_thinking implÃ©mentÃ©
- [ ] `services/workflow/engine.py` project_id corrigÃ©
- [ ] `services/knowledge/context_builder.py` max_tokens ajoutÃ©
- [ ] Feature flags configurÃ©s

### Frontend

- [ ] `stores/workflowStore.ts` WebSocket supprimÃ©
- [ ] `stores/chatStore.ts` crÃ©Ã©
- [ ] `hooks/useStreamingResponse.ts` crÃ©Ã©
- [ ] `hooks/useMetrics.ts` crÃ©Ã©
- [ ] `components/common/ThinkingIndicator.tsx` crÃ©Ã©
- [ ] `components/common/StreamingText.tsx` crÃ©Ã©
- [ ] `components/workflow/WorkflowStepContainer.tsx` mis Ã  jour
- [ ] `i18n/translations.ts` clÃ©s ajoutÃ©es

### Tests

- [ ] Unit tests: metrics, streaming hook, stores
- [ ] Integration tests: single call, SSE
- [ ] E2E tests: workflow streaming

### Documentation

- [ ] CHANGELOG mis Ã  jour
- [ ] README mis Ã  jour si nÃ©cessaire
- [ ] MÃ©triques baseline documentÃ©es

---

## Notes

1. **PrioritÃ© au TTFT** : La vitesse perÃ§ue (premier feedback visible) est plus importante que la vitesse totale.

2. **Instrumenter d'abord** : Impossible d'amÃ©liorer sans mesurer. Toujours baseline â†’ change â†’ measure.

3. **Feature flags** : Permettent un rollout progressif et un rollback rapide si problÃ¨me.

4. **Streaming = UX** : L'impact psychologique du feedback immÃ©diat est majeur pour l'expÃ©rience utilisateur.

5. **Contexte compact** : Moins de tokens = moins cher + plus rapide + rÃ©ponses souvent meilleures (moins de distraction).

---

## RÃ©fÃ©rences

- [P2.10 - LLM Performance Architecture](./P2.10-llm-performance-architecture.md)
- [TASK - Workflow Step Architecture](./TASK-workflow-step-architecture.md)
- [Server-Sent Events (MDN)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)
- [Anthropic Streaming](https://docs.anthropic.com/en/api/streaming)
